# Story 1.11: Data Quality Validation & Monitoring

## Status
Ready

## Story
**As a** developer,
**I want** data quality checks and monitoring during ingestion,
**so that** I can detect and fix issues early.

## Acceptance Criteria
1. `src/ingestion/validators.py` created with validation functions
2. Validation checks:
   - **XML validation:** Well-formed XML, expected schema
   - **Markdown quality:** Non-empty content, valid frontmatter
   - **Chunk quality:** Min/max token count, non-empty text
   - **Embedding quality:** Correct dimensions (1536), no NaN values
   - **Metadata completeness:** Required fields present
3. Validation metrics logged:
   - Articles skipped (malformed XML)
   - Chunks discarded (too short/long)
   - Embedding failures (API errors)
   - Metadata extraction failures
4. Quality report generated:
   - `data/quality-report.json` with summary statistics
5. Alerting for critical issues:
   - >5% articles skipped → warning
   - >10% embeddings failed → error
6. Unit tests for each validation function
7. Integration test with intentionally malformed data

## Tasks / Subtasks

- [ ] Create validators module (AC: 1)
  - [ ] Create src/ingestion/validators.py
  - [ ] Add structlog logger instance
  - [ ] Add type hints and docstrings
- [ ] Implement XML validation (AC: 2)
  - [ ] Implement validate_xml(xml_path: str) -> ValidationResult
  - [ ] Check file exists and readable
  - [ ] Validate XML is well-formed (parse without error)
  - [ ] Validate expected namespace
  - [ ] Check for required elements (<page>, <title>, <text>)
  - [ ] Return ValidationResult with status and issues
- [ ] Implement markdown validation (AC: 2)
  - [ ] Implement validate_markdown(markdown: str) -> ValidationResult
  - [ ] Check content is non-empty
  - [ ] Validate frontmatter is valid YAML
  - [ ] Check required frontmatter fields (title, wiki_id)
  - [ ] Check content has actual text (not just frontmatter)
  - [ ] Return ValidationResult with status and issues
- [ ] Implement chunk validation (AC: 2)
  - [ ] Implement validate_chunk(chunk: Chunk) -> ValidationResult
  - [ ] Check chunk_text is non-empty
  - [ ] Validate token count >= MIN_TOKENS (50)
  - [ ] Validate token count <= MAX_TOKENS (500)
  - [ ] Check required fields present (article_title, section_path)
  - [ ] Return ValidationResult with status and issues
- [ ] Implement embedding validation (AC: 2)
  - [ ] Implement validate_embedding(embedding: np.ndarray) -> ValidationResult
  - [ ] Check dimensions == 1536
  - [ ] Check no NaN values (np.isnan)
  - [ ] Check no Inf values (np.isinf)
  - [ ] Check dtype is float32 or float64
  - [ ] Optionally check L2 norm is close to 1.0 (normalized)
  - [ ] Return ValidationResult with status and issues
- [ ] Implement metadata validation (AC: 2)
  - [ ] Implement validate_metadata(metadata: Dict) -> ValidationResult
  - [ ] Check required fields present:
    - article_title
    - section_path
    - chunk_index
    - content_type
    - spoiler_flag
  - [ ] Check field types are correct
  - [ ] Check optional fields if present (faction, era)
  - [ ] Return ValidationResult with status and issues
- [ ] Create ValidationResult dataclass
  - [ ] Define ValidationResult with fields:
    - valid: bool
    - issues: List[str]
    - warnings: List[str]
  - [ ] Add helper methods: is_valid(), has_warnings()
- [ ] Implement metrics tracking (AC: 3)
  - [ ] Create ValidationMetrics dataclass:
    - articles_processed: int
    - articles_skipped: int
    - chunks_created: int
    - chunks_discarded: int
    - embeddings_generated: int
    - embedding_failures: int
    - metadata_extractions: int
    - metadata_failures: int
  - [ ] Track metrics during pipeline execution
  - [ ] Log metrics at intervals (every 100 articles)
- [ ] Implement quality report generation (AC: 4)
  - [ ] Implement generate_quality_report(metrics: ValidationMetrics, output_path: str)
  - [ ] Calculate percentages (skip rate, failure rate)
  - [ ] Calculate quality score (0-100)
  - [ ] Format as JSON
  - [ ] Save to data/quality-report.json
  - [ ] Include timestamp and metadata
- [ ] Implement alerting (AC: 5)
  - [ ] Implement check_quality_thresholds(metrics: ValidationMetrics)
  - [ ] Calculate skip percentage
  - [ ] Calculate failure percentage
  - [ ] If >5% articles skipped → log warning
  - [ ] If >10% embeddings failed → log error
  - [ ] Return alert level (none, warning, error)
- [ ] Integrate validation into pipeline
  - [ ] Add validation calls to IngestionPipeline
  - [ ] Validate XML before parsing
  - [ ] Validate markdown after conversion
  - [ ] Validate chunks after chunking
  - [ ] Validate embeddings after generation
  - [ ] Validate metadata after extraction
  - [ ] Track metrics throughout pipeline
  - [ ] Generate quality report at end
- [ ] Write unit tests (AC: 6)
  - [ ] Create tests/unit/test_validators.py
  - [ ] Test validate_xml with valid and invalid XML
  - [ ] Test validate_markdown with valid and invalid markdown
  - [ ] Test validate_chunk with valid and invalid chunks
  - [ ] Test validate_embedding with valid and invalid embeddings
  - [ ] Test validate_metadata with valid and invalid metadata
  - [ ] Test metrics tracking
  - [ ] Test quality report generation
  - [ ] Test alerting thresholds
- [ ] Write integration test (AC: 7)
  - [ ] Create tests/integration/test_validation.py
  - [ ] Create intentionally malformed test data:
    - Invalid XML
    - Empty markdown
    - Chunks too short/long
    - NaN embeddings
    - Missing metadata fields
  - [ ] Run validation on malformed data
  - [ ] Verify issues detected correctly
  - [ ] Verify metrics tracked correctly
  - [ ] Verify alerts triggered correctly
- [ ] Verify all acceptance criteria met
  - [ ] Run all tests with `poetry run pytest`
  - [ ] Test with real pipeline data
  - [ ] Verify quality report is meaningful
  - [ ] Verify alerts work correctly

## Dev Notes

### Previous Story Insights
Story 1.10 completed:
- Ingestion pipeline refactored for markdown-first architecture
- Pipeline reads from markdown archive (not XML directly)
- Change detection implemented with `last_updated` frontmatter
- Ready for quality validation

**Note (2025-12-31):** With markdown-first architecture:
- XML validation only needed during `parse-wiki` (data preparation), not during ingestion
- Markdown validation focuses on frontmatter (title, wiki_id, last_updated) and content
- Ingestion pipeline validates markdown files as they are loaded by `MarkdownLoader`

### Technical Notes
**[Source: docs/epic-1-foundation-data-pipeline.md - Story 1.11]**

- Validation should be fast (don't block pipeline)
- Log detailed errors for debugging
- Quality report useful for monitoring data drift over time

### ValidationResult Pattern

```python
from dataclasses import dataclass
from typing import List

@dataclass
class ValidationResult:
    valid: bool
    issues: List[str]
    warnings: List[str]

    def is_valid(self) -> bool:
        return self.valid and len(self.issues) == 0

    def has_warnings(self) -> bool:
        return len(self.warnings) > 0
```

### Quality Report Format

```json
{
  "timestamp": "2024-09-20T12:00:00Z",
  "metrics": {
    "articles_processed": 10234,
    "articles_skipped": 45,
    "skip_rate": 0.44,
    "chunks_created": 51234,
    "chunks_discarded": 234,
    "discard_rate": 0.46,
    "embeddings_generated": 51000,
    "embedding_failures": 12,
    "failure_rate": 0.02,
    "metadata_extractions": 51234,
    "metadata_failures": 8
  },
  "quality_score": 98.5,
  "alerts": [
    {
      "level": "warning",
      "message": "5.2% of articles skipped due to parsing errors"
    }
  ],
  "top_issues": [
    "Malformed XML in 25 articles",
    "Empty content in 20 articles",
    "Token limit exceeded in 15 chunks"
  ]
}
```

### Validation Integration in Pipeline

```python
# In MarkdownLoader.load_file():
def load_file(file_path):
    content = file_path.read_text()
    frontmatter, markdown_content = parse_frontmatter(content)

    # Validate frontmatter (required fields: title, wiki_id, last_updated)
    if not validate_frontmatter(frontmatter):
        logger.warning("Invalid frontmatter", file_path=file_path)
        return None  # Skip file

    return WikiArticle(...)

# In pipeline._process_batch():
def process_article(article):
    # Validate markdown content quality
    markdown_validation = validate_markdown(article.content)
    if not markdown_validation.is_valid():
        logger.warning("Invalid markdown", issues=markdown_validation.issues)
        stats.articles_failed += 1
        return

    # Chunk article
    chunks = chunker.chunk_markdown(article.content, article.title)

    # Validate chunks (token count, non-empty)
    valid_chunks = []
    for chunk in chunks:
        chunk_validation = validate_chunk(chunk)
        if chunk_validation.is_valid():
            valid_chunks.append(chunk)
        else:
            logger.warning("Invalid chunk", issues=chunk_validation.issues)
            metrics.chunks_discarded += 1

    # Continue with valid chunks...
```

**Note:** XML validation is handled separately in `parse-wiki` command (data preparation step).

### Quality Thresholds

**Skip rate thresholds:**
- <1%: Excellent
- 1-5%: Good
- 5-10%: Warning
- >10%: Error

**Failure rate thresholds:**
- <0.1%: Excellent
- 0.1-1%: Good
- 1-10%: Warning
- >10%: Error

### Embedding Validation

```python
def validate_embedding(embedding: np.ndarray) -> ValidationResult:
    issues = []

    # Check dimensions
    if embedding.shape[0] != 1536:
        issues.append(f"Invalid dimensions: {embedding.shape[0]} (expected 1536)")

    # Check for NaN
    if np.isnan(embedding).any():
        issues.append("Embedding contains NaN values")

    # Check for Inf
    if np.isinf(embedding).any():
        issues.append("Embedding contains Inf values")

    # Check normalization (optional)
    norm = np.linalg.norm(embedding)
    if not (0.99 <= norm <= 1.01):
        issues.append(f"Embedding not normalized: L2 norm = {norm}")

    return ValidationResult(
        valid=len(issues) == 0,
        issues=issues,
        warnings=[]
    )
```

### Coding Standards Reminders

**[Source: docs/architecture/coding-standards.md]**
- Type hints required for all function signatures
- Docstrings required for all public functions (Google style)
- Use dataclasses for structured results
- Log validation failures with context
- Don't block pipeline on non-critical issues

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
None - Implementation completed without blockers

### Completion Notes List
- Created validators.py module with all validation functions
- Implemented ValidationResult dataclass with helper methods
- Implemented ValidationMetrics dataclass with rate calculations
- Implemented validate_xml() for XML structure validation
- Implemented validate_markdown() for content quality validation
- Implemented validate_chunk() for chunk token count validation
- Implemented validate_embedding() for embedding quality validation
- Implemented validate_metadata() for metadata completeness validation
- **Note:** Validation integration into pipeline not yet implemented
- **Note:** Quality report generation and alerting not yet implemented
- **Note:** Unit and integration tests not yet written

### File List
- src/ingestion/validators.py (new)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-26 | 1.0 | Story created from Epic 1 | Bob (Scrum Master) |
| 2025-12-29 | 1.1 | Core validation module implemented | James (Dev Agent) |
| 2025-12-31 | 1.2 | Updated notes for markdown-first architecture | Claude Opus 4.5 |

## QA Results
[To be populated by QA agent]
