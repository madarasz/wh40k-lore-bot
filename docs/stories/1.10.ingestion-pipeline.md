# Story 1.10: Complete Ingestion Pipeline Orchestration

## Status
Draft

## Story
**As a** developer,
**I want** an orchestrated pipeline that runs all ingestion steps end-to-end,
**so that** I can transform the wiki XML export into a searchable vector database with a single command.

## Acceptance Criteria
1. `src/ingestion/pipeline.py` created with `IngestionPipeline` class
2. Pipeline executes steps in order:
   1. Parse XML → markdown articles
   2. Save markdown files to archive
   3. Chunk markdown articles
   4. Extract metadata from chunks
   5. Generate embeddings (batched)
   6. Store chunks + embeddings + metadata in Chroma
3. Progress tracking:
   - Log completion of each major step
   - Track articles processed, chunks created, embeddings generated
   - Display estimated time remaining
4. Resumable pipeline:
   - Track progress in SQLite: `ingestion_progress` table
   - If pipeline fails, resume from last completed batch
5. CLI command: `poetry run ingest-wiki data/warhammer40k_pages_current.xml`
6. CLI options:
   - `--batch-size`: Number of articles per batch (default: 100)
   - `--skip-existing`: Skip already processed articles
   - `--dry-run`: Parse and chunk without generating embeddings
   - `--page-ids-file`: Path to file containing page IDs to process (e.g., `data/test-bed-pages.txt`)
7. Summary report at completion:
   - Total articles processed
   - Total chunks created
   - Total embeddings generated
   - Total cost (OpenAI API)
   - Processing time
8. Error handling:
   - Log errors to `logs/ingestion-errors.log`
   - Continue processing on non-fatal errors
   - Rollback batch on fatal errors
9. Integration test: Full pipeline with small XML sample (5 articles)
10. End-to-end test: Full 173MB XML file (optional, takes 2-4 hours)

## Tasks / Subtasks

- [ ] Create IngestionPipeline class (AC: 1)
  - [ ] Create src/ingestion/pipeline.py
  - [ ] Implement IngestionPipeline class
  - [ ] Initialize all component classes:
    - WikiXMLParser
    - MarkdownChunker
    - MetadataExtractor
    - EmbeddingGenerator
    - ChromaVectorStore
  - [ ] Add structlog logger instance
  - [ ] Add type hints and docstring
- [ ] Create progress tracking table (AC: 4)
  - [ ] Create Alembic migration for ingestion_progress table
  - [ ] Define columns:
    - id (UUID)
    - article_id (string, indexed)
    - status (enum: pending, processing, completed, failed)
    - batch_number (integer)
    - processed_at (timestamp)
    - error_message (text, nullable)
  - [ ] Create ORM model for IngestionProgress
  - [ ] Create repository for progress tracking
- [ ] Implement pipeline orchestration (AC: 2)
  - [ ] Implement run(xml_path, page_ids=None, batch_size=100, skip_existing=False, dry_run=False)
  - [ ] Step 1: Parse XML with WikiXMLParser
  - [ ] Step 2: Save markdown files to archive
  - [ ] Step 3: Chunk markdown with MarkdownChunker
  - [ ] Step 4: Extract metadata with MetadataExtractor
  - [ ] Step 5: Generate embeddings (unless dry_run)
  - [ ] Step 6: Store in Chroma (unless dry_run)
  - [ ] Process in batches of batch_size articles
  - [ ] Track progress after each batch
- [ ] Implement progress tracking (AC: 3, 4)
  - [ ] Check ingestion_progress table before processing
  - [ ] Skip articles with status=completed if skip_existing=True
  - [ ] Mark articles as processing before batch
  - [ ] Mark articles as completed after successful batch
  - [ ] Mark articles as failed on errors
  - [ ] Log batch completion with stats
  - [ ] Calculate and display estimated time remaining
- [ ] Implement resumable pipeline (AC: 4)
  - [ ] On startup, check for incomplete batches
  - [ ] Resume from last completed batch
  - [ ] Rollback incomplete batch on startup
  - [ ] Log resume information
- [ ] Implement error handling (AC: 8)
  - [ ] Configure file handler for logs/ingestion-errors.log
  - [ ] Wrap each batch in try/except
  - [ ] Log errors with article context
  - [ ] Continue on non-fatal errors (skip article)
  - [ ] Rollback batch on fatal errors
  - [ ] Update progress table with error messages
- [ ] Implement summary report (AC: 7)
  - [ ] Track statistics during pipeline:
    - Total articles processed
    - Total chunks created
    - Total embeddings generated
    - Total tokens used
    - Total cost
    - Start time, end time
  - [ ] Display summary at completion
  - [ ] Save summary to logs/ingestion-summary.json
- [ ] Create CLI command (AC: 5, 6)
  - [ ] Create src/cli/ingest_wiki.py
  - [ ] Use click for CLI framework
  - [ ] Add xml_path argument (required)
  - [ ] Add --batch-size option (default: 100)
  - [ ] Add --skip-existing flag
  - [ ] Add --dry-run flag
  - [ ] Add --page-ids-file option
  - [ ] Load page IDs from file if provided
  - [ ] Display progress bar with tqdm
  - [ ] Add to pyproject.toml [tool.poetry.scripts]
- [ ] Write unit tests
  - [ ] Create tests/unit/test_ingestion_pipeline.py
  - [ ] Mock all component classes
  - [ ] Test pipeline orchestration flow
  - [ ] Test batch processing
  - [ ] Test progress tracking
  - [ ] Test error handling
  - [ ] Test resume logic
  - [ ] Test dry_run mode
  - [ ] Test skip_existing mode
- [ ] Write integration test (AC: 9)
  - [ ] Create tests/integration/test_pipeline_integration.py
  - [ ] Create small XML sample (5 articles)
  - [ ] Run full pipeline end-to-end
  - [ ] Verify markdown files created
  - [ ] Verify chunks in database
  - [ ] Verify embeddings in Chroma
  - [ ] Verify metadata populated
  - [ ] Clean up test data after test
- [ ] Optional: End-to-end test (AC: 10)
  - [ ] Document how to run full wiki ingestion
  - [ ] Test with full 173MB XML file
  - [ ] Monitor memory usage
  - [ ] Monitor processing time
  - [ ] Verify <500MB memory footprint
  - [ ] Verify 2-4 hour processing time
- [ ] Verify all acceptance criteria met
  - [ ] Run all tests with `poetry run pytest`
  - [ ] Test CLI command with sample XML
  - [ ] Test with --page-ids-file option
  - [ ] Test resume after interruption
  - [ ] Verify summary report is accurate

## Dev Notes

### Previous Story Insights
All component stories completed:
- Story 1.4: WikiXMLParser ready
- Story 1.6: MarkdownChunker ready
- Story 1.7: EmbeddingGenerator ready
- Story 1.8: ChromaVectorStore ready
- Story 1.9: MetadataExtractor ready

### Technical Notes
**[Source: Epic 1 Story 1.10]**

- Use `click` library for CLI
- Batch processing to manage memory (process 100 articles at a time)
- Transactional batches: Commit to DB only after successful embedding generation
- Expected total processing time: 2-4 hours

### Pipeline Architecture

```
XML File
   ↓
WikiXMLParser → Articles
   ↓
MarkdownChunker → Chunks
   ↓
MetadataExtractor → Metadata
   ↓
EmbeddingGenerator → Embeddings
   ↓
ChromaVectorStore → Vector DB
```

### Batch Processing Flow

```python
def run(xml_path, page_ids=None, batch_size=100):
    # Parse articles
    articles = parser.parse_xml_export(xml_path, page_ids)

    # Process in batches
    for batch_num, batch in enumerate(batched(articles, batch_size)):
        try:
            # Chunk articles
            all_chunks = []
            for article in batch:
                chunks = chunker.chunk_markdown(article.content, article.title)
                all_chunks.extend(chunks)

            # Extract metadata
            for chunk in all_chunks:
                chunk.metadata = extractor.extract_metadata(chunk)

            # Generate embeddings
            embeddings = generator.generate_embeddings([c.chunk_text for c in all_chunks])

            # Store in Chroma
            vector_store.add_chunks(all_chunks, embeddings)

            # Mark batch complete
            mark_batch_completed(batch_num, len(batch), len(all_chunks))

        except Exception as e:
            logger.error("Batch failed", batch_num=batch_num, error=str(e))
            mark_batch_failed(batch_num, str(e))
            # Continue to next batch
```

### Progress Tracking Schema

```python
class IngestionProgress(Base):
    __tablename__ = "ingestion_progress"

    id: UUID
    article_id: str  # wiki page id
    status: str  # pending, processing, completed, failed
    batch_number: int
    processed_at: datetime
    error_message: Optional[str]
```

### Resumable Pipeline Logic

```python
def resume_from_last_batch():
    # Find last completed batch
    last_batch = get_last_completed_batch()

    if last_batch:
        logger.info(f"Resuming from batch {last_batch + 1}")
        # Rollback any incomplete batches
        rollback_incomplete_batches()
        return last_batch + 1

    return 0  # Start from beginning
```

### CLI Usage Examples

```bash
# Basic usage
poetry run ingest-wiki data/warhammer40k_pages_current.xml

# With test bed (100 pages)
poetry run ingest-wiki data/warhammer40k_pages_current.xml --page-ids-file data/test-bed-pages.txt

# Dry run (no embeddings)
poetry run ingest-wiki data/warhammer40k_pages_current.xml --dry-run

# Resume after interruption
poetry run ingest-wiki data/warhammer40k_pages_current.xml --skip-existing

# Smaller batches
poetry run ingest-wiki data/warhammer40k_pages_current.xml --batch-size 50
```

### Summary Report Format

```json
{
  "start_time": "2024-09-20T10:00:00Z",
  "end_time": "2024-09-20T12:30:00Z",
  "duration_seconds": 9000,
  "articles_processed": 10234,
  "chunks_created": 51234,
  "embeddings_generated": 51234,
  "tokens_used": 12500000,
  "estimated_cost_usd": 0.25,
  "errors": 12,
  "batches_completed": 103
}
```

### Coding Standards Reminders

**[Source: architecture/coding-standards.md]**
- Type hints required for all function signatures
- Docstrings required for all public functions (Google style)
- Use click for CLI
- Use tqdm for progress bars
- Use structlog for logging with context
- Handle errors gracefully (don't crash pipeline)
- Log progress at meaningful intervals

## Dev Agent Record

### Agent Model Used
[To be populated during implementation]

### Debug Log References
[To be populated during implementation]

### Completion Notes List
[To be populated during implementation]

### File List
[To be populated during implementation]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-26 | 1.0 | Story created from Epic 1 | Bob (Scrum Master) |

## QA Results
[To be populated by QA agent]
