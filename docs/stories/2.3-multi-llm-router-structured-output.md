# Story 2.3: Multi-LLM Router, OpenAI & Anthropic Providers with Structured Output

## Status
Complete

## Epic
Epic 2: Core RAG Query System

## Dependencies
- Pydantic ^2.0 (new dependency)
- OpenAI SDK v2.14+ with structured output support
- Anthropic SDK v0.75+ with structured output support
- Existing structlog infrastructure

## Story
**As a** developer,
**I want** a multi-LLM router with a pluggable provider system, OpenAI and Anthropic implementations with structured JSON output using Pydantic validation,
**so that** I can generate validated responses with personality and source attribution.

## Acceptance Criteria

### Structured Output Model
1. `LLMStructuredResponse` Pydantic model created in `src/llm/structured_output.py`:
   - `answer: str | None` (required when smalltalk=false)
   - `personality_reply: str` (always required)
   - `sources: list[HttpUrl] | None` (required when smalltalk=false, full wiki URLs)
   - `smalltalk: bool` (always required)
   - Field validators enforce rules based on smalltalk flag
   - JSON schema generation for server-side structured output

### Provider Base Class
2. `LLMProvider` abstract base class created in `src/llm/base_provider.py`:
   - `async def generate_structured(prompt: str, options: GenerationOptions, response_schema: type[BaseModel]) -> LLMStructuredResponse`
   - `def get_provider_name() -> str`

### Data Classes
3. `GenerationOptions` data class defined:
   - `model`: str (e.g., "gpt-4.1", "claude-sonnet-4-5")
   - `temperature`: float (default: 0.7)
   - `max_tokens`: int (default: 800, increased for structured output)
   - `response_language`: str (default: "hu" for Hungarian)

4. `LLMResponse` data class defined:
   - `text`: str (generated response)
   - `provider`: str (provider name)
   - `model`: str (model used)
   - `tokens_prompt`: int
   - `tokens_completion`: int
   - `cost_usd`: float
   - `latency_ms`: int

### Pricing Calculator
5. `PricingCalculator` class created in `src/llm/pricing.py`:
   - Centralized cost estimation for all LLM and embedding operations
   - `def calculate_cost(model: str, prompt_tokens: int, completion_tokens: int) -> float`
   - Supports all models: `gpt-4.1`, `claude-sonnet-4-5`, `claude-haiku-4-5`, `claude-opus-4-5`, `text-embedding-3-small`
   - Used by LLM providers and `src/ingestion/embedding_generator.py`

### OpenAI Provider
6. `OpenAIProvider` implementation in `src/llm/providers/openai_provider.py`:
   - Uses `openai` SDK v2.14+ with structured output support
   - Implements all base class methods
   - API key from environment: `OPENAI_API_KEY`
   - Supported model: `gpt-4.1`
   - **Structured Output Strategy:**
     - Primary: Use `beta.chat.completions.parse(response_format=PydanticModel)`
     - Fallback: Use `response_format={"type": "json_schema"}` + client-side Pydantic validation
     - Graceful degradation if beta API unavailable

### Anthropic Provider
7. `AnthropicProvider` implementation in `src/llm/providers/anthropic_provider.py`:
   - Uses `anthropic` SDK v0.75+ with structured output support
   - Implements all base class methods
   - API key from environment: `ANTHROPIC_API_KEY`
   - Supported models: `claude-sonnet-4-5`, `claude-haiku-4-5`, `claude-opus-4-5`
   - **Structured Output Strategy:**
     - Primary: Use `beta.messages.parse(response_model=PydanticModel)` with header `anthropic-beta: structured-outputs-2025-11-13`
     - Uses constrained decoding (grammar-based, cannot produce invalid JSON)
     - Fallback: Use tool use pattern + client-side Pydantic validation
     - 100-300ms overhead for grammar compilation (cached 24 hours)

### Retry Logic
8. `generate_structured()` implementation with retry logic:
   - Exponential backoff: wait times [2s, 4s, 8s] for 3 retries
   - Retry on: `RateLimitError`, `APIConnectionError`, `APITimeoutError`
   - No retry on: `AuthenticationError`, `InvalidRequestError`, `ValidationError`
   - Fail-fast on Pydantic ValidationError (LLM returned invalid schema)
   - Log each retry attempt with context

### Prompt Templates
9. Prompt template system in `prompts/` directory:
   - `prompts/system.md` - System prompt template with:
     - Purpose: WH40K lore expert description
     - Reasoning instructions for the model
     - Output JSON structure definition
     - Constraints and formatting rules
     - `{persona}` placeholder (loaded from `prompts/persona.md`)
     - `{language}` placeholder for response language
   - `prompts/user.md` - User prompt template with:
     - `{chunks}` placeholder for retrieved context
     - `{question}` placeholder for user's question
   - `prompts/persona.md` - Bot personality definition
   - `PromptBuilder` class in `src/llm/prompt_builder.py` to load and render templates

### Multi-LLM Router
10. `MultiLLMRouter` class in `src/llm/llm_router.py`:
    - Provider registry: Maps provider name → LLMProvider instance
    - **Auto-detect provider from model name:**
      - `claude-*` → AnthropicProvider
      - `gpt-*` → OpenAIProvider
    - `async def generate_structured(prompt: str, options: GenerationOptions, response_schema: type[BaseModel]) -> LLMStructuredResponse`
    - Default model from environment: `LLM_DEFAULT_MODEL` (default: "claude-sonnet-4-5")
    - No fallback logic: Fail fast if provider fails

### Observability
11. Structured logging:
    - Provider name, model, tokens, cost, latency
    - Structured output validation success/failure
    - Server-side vs client-side validation path taken
    - Retry attempts with errors
    - Final success/failure status

### Testing
12. Unit tests covering:
    - Pydantic model validation (smalltalk=true/false cases)
    - PricingCalculator accuracy for all models
    - OpenAI provider: Successful structured generation (beta.chat.completions.parse)
    - OpenAI provider: Client-side fallback (response_format + manual validation)
    - Anthropic provider: Successful structured generation (beta.messages.parse)
    - Anthropic provider: Client-side fallback (tool use pattern)
    - Retry logic: Retryable errors (3 retries, then fail)
    - Retry logic: Non-retryable errors (fail immediately, including ValidationError)
    - Prompt template loading and rendering
    - Multi-LLM router: Model-based provider selection
    - Multi-LLM router: Unknown model error
    - Graceful degradation from server-side to client-side validation

## Tasks / Subtasks

- [x] Task 1: Add Pydantic dependency (AC: Dependencies)
  - [x] Add `pydantic = "^2.0"` to `pyproject.toml`
  - [x] Run `poetry lock && poetry install`
  - [x] Verify Pydantic v2 installed

- [x] Task 2: Create structured output Pydantic model (AC: 1)
  - [x] Create `src/llm/` module with `__init__.py`
  - [x] Create `src/llm/structured_output.py`
  - [x] Define `LLMStructuredResponse` with fields
  - [x] Add Pydantic validators for smalltalk-based field requirements
  - [x] Add JSON schema generation method
  - [x] Test model validation with various inputs

- [x] Task 3: Create base provider and data classes (AC: 2, 3, 4)
  - [x] Update `src/llm/base_provider.py`
  - [x] Remove `supports_language()` abstract method
  - [x] Remove `estimate_cost()` abstract method
  - [x] Update `GenerationOptions` dataclass (remove `use_structured_output`, `system_prompt`)
  - [x] Keep `LLMResponse` dataclass unchanged

- [x] Task 4: Create PricingCalculator (AC: 5)
  - [x] Create `src/llm/pricing.py`
  - [x] Implement `PricingCalculator` class with model pricing data
  - [x] Add pricing for: `gpt-4.1`, `claude-sonnet-4-5`, `claude-haiku-4-5`, `claude-opus-4-5`, `text-embedding-3-small`
  - [x] Update `src/ingestion/embedding_generator.py` to use PricingCalculator
  - [x] Update LLM providers to use PricingCalculator

- [x] Task 5: Update OpenAI provider (AC: 6, 8)
  - [x] Update `src/llm/providers/openai_provider.py`
  - [x] Remove `supports_language()` method
  - [x] Remove `estimate_cost()` method (use PricingCalculator)
  - [x] Remove language instruction logic
  - [x] Update supported model to `gpt-4.1`
  - [x] Remove `OPENAI_QUERY_MODEL` env var usage
  - [x] Keep retry logic with exponential backoff

- [x] Task 6: Update Anthropic provider (AC: 7, 8)
  - [x] Update `src/llm/providers/anthropic_provider.py`
  - [x] Remove `supports_language()` method
  - [x] Remove `estimate_cost()` method (use PricingCalculator)
  - [x] Remove language instruction logic
  - [x] Update supported models to `claude-sonnet-4-5`, `claude-haiku-4-5`, `claude-opus-4-5`
  - [x] Remove `ANTHROPIC_QUERY_MODEL` env var usage
  - [x] Keep retry logic with exponential backoff

- [x] Task 7: Create Prompt Templates (AC: 9)
  - [x] Create `prompts/` directory at project root
  - [x] Create `prompts/system.md` with placeholders
  - [x] Create `prompts/user.md` with placeholders
  - [x] Create `prompts/persona.md` with bot personality
  - [x] Create `src/llm/prompt_builder.py` with `PromptBuilder` class
  - [x] Implement template loading and rendering

- [x] Task 8: Update Multi-LLM Router (AC: 10)
  - [x] Update `src/llm/llm_router.py`
  - [x] Implement model-to-provider auto-detection
  - [x] Remove `LLM_DEFAULT_PROVIDER` env var usage
  - [x] Add `LLM_DEFAULT_MODEL` env var (default: "claude-sonnet-4-5")
  - [x] Update provider selection logic

- [x] Task 9: Update environment configuration
  - [x] Update `.env.example`:
    - Keep `OPENAI_API_KEY`
    - Keep `ANTHROPIC_API_KEY`
    - Remove `OPENAI_QUERY_MODEL`
    - Remove `ANTHROPIC_QUERY_MODEL`
    - Remove `LLM_DEFAULT_PROVIDER`
    - Add `LLM_DEFAULT_MODEL`

- [x] Task 10: Update unit tests (AC: 12)
  - [x] Update tests to remove language support tests
  - [x] Add PricingCalculator tests
  - [x] Add prompt template tests
  - [x] Update router tests for model-based selection
  - [x] Update provider tests for new model names

- [ ] Task 11: Integration testing (deferred - requires real API keys)
  - [ ] Test OpenAI provider with real API (mark as integration test)
  - [ ] Test Anthropic provider with real API (mark as integration test)
  - [ ] Verify structured output validation end-to-end
  - [ ] Verify retry logic with simulated failures

## Dev Notes

### Technical Background
- **Strategy Pattern:** Enables future provider extensibility (Gemini, Grok, Mistral)
- **Pydantic Scope:** ONLY for LLM responses; existing dataclasses remain unchanged
- **Server-Side Benefits:** Reduces latency and improves reliability
- **Anthropic Advantage:** Constrained decoding is most reliable (grammar guarantees)
- **OpenAI Limitation:** beta.chat.completions.parse functional but has occasional validation errors
- **Fail-Fast Philosophy:** No automatic fallbacks to prevent unexpected behavior
- **Retry Strategy:** Essential for production reliability (handles transient API failures)
- **Model-Based Routing:** Router auto-detects provider from model name prefix

### Pydantic Model Example
```python
from pydantic import BaseModel, HttpUrl, field_validator

class LLMStructuredResponse(BaseModel):
    """Structured LLM response with validation."""
    answer: str | None = None
    personality_reply: str
    sources: list[HttpUrl] | None = None
    smalltalk: bool

    @field_validator('answer')
    def answer_required_for_lore(cls, v, info):
        if not info.data.get('smalltalk') and not v:
            raise ValueError('answer required when smalltalk=false')
        return v

    @field_validator('sources')
    def sources_required_for_lore(cls, v, info):
        if not info.data.get('smalltalk') and not v:
            raise ValueError('sources required when smalltalk=false')
        return v
```

### PricingCalculator Example
```python
class PricingCalculator:
    """Centralized pricing for all LLM and embedding operations."""

    # Pricing per 1K tokens (USD)
    PRICING = {
        # OpenAI
        "gpt-4.1": {"input": 0.002, "output": 0.008},
        # Anthropic
        "claude-sonnet-4-5": {"input": 0.003, "output": 0.015},
        "claude-haiku-4-5": {"input": 0.0008, "output": 0.004},
        "claude-opus-4-5": {"input": 0.015, "output": 0.075},
        # Embeddings
        "text-embedding-3-small": {"input": 0.00002, "output": 0.0},
    }

    def calculate_cost(
        self, model: str, prompt_tokens: int, completion_tokens: int
    ) -> float:
        """Calculate cost for a generation request."""
        pricing = self.PRICING.get(model)
        if not pricing:
            raise ValueError(f"Unknown model: {model}")

        input_cost = (prompt_tokens / 1000) * pricing["input"]
        output_cost = (completion_tokens / 1000) * pricing["output"]
        return input_cost + output_cost
```

### Router Model Detection Example
```python
class MultiLLMRouter:
    """Multi-LLM router with model-based provider selection."""

    def __init__(self) -> None:
        self.providers: dict[str, LLMProvider] = {}
        self.default_model = os.getenv("LLM_DEFAULT_MODEL", "claude-sonnet-4-5")

        # Initialize providers
        self._register_provider("openai", OpenAIProvider())
        self._register_provider("anthropic", AnthropicProvider())

    def _get_provider_for_model(self, model: str) -> LLMProvider:
        """Auto-detect provider from model name."""
        if model.startswith("claude-"):
            return self.providers["anthropic"]
        elif model.startswith("gpt-"):
            return self.providers["openai"]
        else:
            raise LLMProviderError(f"Unknown model: {model}")
```

### Prompt Template Examples

**prompts/system.md:**
```
You are an expert in Warhammer 40,000 lore, answering questions based on provided context.

## Persona
{persona}

## Response Format
You must respond in valid JSON with this structure:
- answer: Your detailed answer based on the context (null if smalltalk)
- personality_reply: A brief, in-character response
- sources: List of source URLs from the context (null if smalltalk)
- smalltalk: true if this is casual conversation, false if lore question

## Constraints
- Only use information from the provided context
- If the context doesn't contain the answer, say so
- Always stay in character as defined in the persona
- Respond in {language}

## Reasoning
Think step by step:
1. Determine if this is a lore question or smalltalk
2. If lore, find relevant information in the context
3. Formulate your answer with sources
4. Add a personality touch
```

**prompts/user.md:**
```
## Context
{chunks}

## Question
{question}
```

**prompts/persona.md:**
```
You are a veteran Imperial scholar with deep knowledge of the Imperium's history.
Speak with gravitas and occasional dry humor. Reference the Emperor with reverence.
```

### Retry Logic Pattern
```python
async def _retry_with_backoff(
    self,
    func: Callable,
    *args,
    **kwargs
) -> Any:
    """Retry with exponential backoff."""
    wait_times = [2, 4, 8]  # seconds

    for attempt, wait in enumerate(wait_times, start=1):
        try:
            return await func(*args, **kwargs)
        except (RateLimitError, APIConnectionError, APITimeoutError) as e:
            if attempt == len(wait_times):
                raise
            logger.warning("retrying_llm_call", attempt=attempt, wait=wait, error=str(e))
            await asyncio.sleep(wait)
        except (AuthenticationError, InvalidRequestError, ValidationError):
            # Fail fast on non-retryable errors
            raise
```

### Environment Configuration
Add to `.env.example`:
```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-...

# Anthropic Configuration
ANTHROPIC_API_KEY=sk-ant-...

# LLM Router Configuration
LLM_DEFAULT_MODEL=claude-sonnet-4-5    # Options: claude-sonnet-4-5, claude-haiku-4-5, claude-opus-4-5, gpt-4.1
```

### Relevant Source Tree
```
src/llm/
├── __init__.py                          # LLM module exports
├── structured_output.py                 # Pydantic models
├── base_provider.py                     # Abstract base class
├── llm_router.py                        # MultiLLMRouter
├── pricing.py                           # NEW: PricingCalculator
├── prompt_builder.py                    # NEW: PromptBuilder
├── providers/
│   ├── __init__.py                      # Provider exports
│   ├── openai_provider.py               # OpenAIProvider
│   └── anthropic_provider.py            # AnthropicProvider
prompts/                                 # NEW: Prompt templates
├── system.md                            # NEW: System prompt template
├── user.md                              # NEW: User prompt template
├── persona.md                           # NEW: Bot persona
tests/unit/llm/
├── __init__.py
├── test_structured_output.py            # Pydantic model tests
├── test_pricing.py                      # NEW: PricingCalculator tests
├── test_prompt_builder.py               # NEW: PromptBuilder tests
├── test_openai_provider.py              # OpenAI tests
├── test_anthropic_provider.py           # Anthropic tests
├── test_llm_router.py                   # Router tests
tests/integration/
├── test_llm_providers.py                # Real API tests (integration)
```

### Testing Strategy
- **Unit Tests:** Mock OpenAI and Anthropic SDKs
- **Integration Tests:** Use real APIs (mark with `@pytest.mark.integration`)
- **Cost Awareness:** Integration tests cost real money (use minimal tokens)
- **Validation Testing:** Test both valid and invalid structured outputs

### Cost Estimation Reference
| Provider | Model | Input (per 1K) | Output (per 1K) |
|----------|-------|----------------|-----------------|
| OpenAI | gpt-4.1 | $0.002 | $0.008 |
| Anthropic | claude-sonnet-4-5 | $0.003 | $0.015 |
| Anthropic | claude-haiku-4-5 | $0.0008 | $0.004 |
| Anthropic | claude-opus-4-5 | $0.015 | $0.075 |
| OpenAI | text-embedding-3-small | $0.00002 | N/A |

Average query: ~500 input tokens, ~200 output tokens
- GPT-4.1: ~$0.0026 per query
- Claude Sonnet 4.5: ~$0.0045 per query
- Claude Haiku 4.5: ~$0.0012 per query

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-05 | 1.0 | Story created from Epic 2 | John (PM Agent) |
| 2025-01-09 | 2.0 | Major revision: Added PricingCalculator, prompt templates, model-based routing, removed language support methods | Dev Agent |

## Dev Agent Record

### Agent Model Used
Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References
N/A

### Pull Request
https://github.com/madarasz/wh40k-lore-bot/pull/20

### Completion Notes List
- **All unit tests pass**: 71 tests covering providers, router, pricing, prompt builder, and structured output
- **Linting clean**: All ruff checks pass
- **Type checking clean**: All mypy checks pass
- **Model-based routing**: Router auto-detects provider from model name prefix (claude-* → Anthropic, gpt-* → OpenAI)
- **PricingCalculator**: Centralized cost calculation used by both providers and embedding generator
- **PromptBuilder**: Template loading with caching for system, user, and persona prompts (using .md extension)
- **Integration tests deferred**: Task 11 requires real API keys and should be run separately with `poetry run pytest -m integration`

### File List
**New Files (created):**
- `src/llm/pricing.py` - Centralized PricingCalculator
- `src/llm/prompt_builder.py` - PromptBuilder for template loading
- `prompts/system.md` - System prompt template
- `prompts/user.md` - User prompt template
- `prompts/persona.md` - Bot persona definition
- `tests/unit/llm/test_pricing.py` - PricingCalculator tests
- `tests/unit/llm/test_prompt_builder.py` - PromptBuilder tests
- `tests/unit/llm/__init__.py` - Test module init
- `src/utils/config.py` - Added `get_required_env()` helper function

**Files Modified:**
- `src/llm/__init__.py` - Added exports for new classes
- `src/llm/base_provider.py` - Removed supports_language, estimate_cost abstract methods
- `src/llm/llm_router.py` - Model-based provider selection via model name prefix
- `src/llm/providers/openai_provider.py` - Removed language support, uses PricingCalculator
- `src/llm/providers/anthropic_provider.py` - Removed language support, uses PricingCalculator
- `src/ingestion/embedding_generator.py` - Uses PricingCalculator
- `.env.example` - Updated environment variables
- `tests/unit/llm/test_openai_provider.py` - Updated tests for new API
- `tests/unit/llm/test_anthropic_provider.py` - Updated tests for new API
- `tests/unit/llm/test_llm_router.py` - Updated tests for model-based selection

## QA Results
_To be populated by QA agent_

---

**Estimated Effort:** 6 hours
