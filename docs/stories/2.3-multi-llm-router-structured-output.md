# Story 2.3: Multi-LLM Router, OpenAI & Anthropic Providers with Structured Output

## Status
Not Started

## Epic
Epic 2: Core RAG Query System

## Dependencies
- Pydantic ^2.0 (new dependency)
- OpenAI SDK v2.14+ with structured output support
- Anthropic SDK v0.75+ with structured output support
- Existing structlog infrastructure

## Story
**As a** developer,
**I want** a multi-LLM router with a pluggable provider system, OpenAI and Anthropic implementations with structured JSON output using Pydantic validation,
**so that** I can generate validated responses with personality and source attribution.

## Acceptance Criteria

### Structured Output Model
1. `LLMStructuredResponse` Pydantic model created in `src/llm/structured_output.py`:
   - `answer: str | None` (required when smalltalk=false)
   - `personality_reply: str` (always required)
   - `sources: list[HttpUrl] | None` (required when smalltalk=false, full wiki URLs)
   - `smalltalk: bool` (always required)
   - Field validators enforce rules based on smalltalk flag
   - JSON schema generation for server-side structured output

### Provider Base Class
2. `LLMProvider` abstract base class created in `src/llm/base_provider.py`:
   - `async def generate(prompt: str, options: GenerationOptions) -> LLMResponse`
   - `async def generate_structured(prompt: str, options: GenerationOptions, response_schema: type[BaseModel]) -> LLMStructuredResponse`
   - `async def estimate_cost(prompt_tokens: int, completion_tokens: int) -> float`
   - `def get_provider_name() -> str`
   - `def supports_language(language: str) -> bool`

### Data Classes
3. `GenerationOptions` data class defined:
   - `model`: str (e.g., "gpt-4o-mini", "gpt-4", "claude-3-5-sonnet-20241022")
   - `temperature`: float (default: 0.7)
   - `max_tokens`: int (default: 800, increased for structured output)
   - `system_prompt`: Optional[str]
   - `response_language`: str (default: "hu" for Hungarian)
   - `use_structured_output`: bool (default: False)

4. `LLMResponse` data class defined (for backward compatibility):
   - `text`: str (generated response)
   - `provider`: str (provider name)
   - `model`: str (model used)
   - `tokens_prompt`: int
   - `tokens_completion`: int
   - `cost_usd`: float
   - `latency_ms`: int

### OpenAI Provider
5. `OpenAIProvider` implementation in `src/llm/providers/openai_provider.py`:
   - Uses `openai` SDK v2.14+ with structured output support
   - Implements all base class methods
   - API key from environment: `OPENAI_API_KEY`
   - Default model from environment: `OPENAI_QUERY_MODEL` (default: "gpt-4o-mini")
   - **Structured Output Strategy:**
     - Primary: Use `beta.chat.completions.parse(response_format=PydanticModel)`
     - Fallback: Use `response_format={"type": "json_schema"}` + client-side Pydantic validation
     - Graceful degradation if beta API unavailable

### Anthropic Provider
6. `AnthropicProvider` implementation in `src/llm/providers/anthropic_provider.py`:
   - Uses `anthropic` SDK v0.75+ with structured output support
   - Implements all base class methods
   - API key from environment: `ANTHROPIC_API_KEY`
   - Default model from environment: `ANTHROPIC_QUERY_MODEL` (default: "claude-3-5-sonnet-20241022")
   - **Structured Output Strategy:**
     - Primary: Use `beta.messages.parse(response_model=PydanticModel)` with header `anthropic-beta: structured-outputs-2025-11-13`
     - Uses constrained decoding (grammar-based, cannot produce invalid JSON)
     - Fallback: Use tool use pattern + client-side Pydantic validation
     - 100-300ms overhead for grammar compilation (cached 24 hours)

### Retry Logic
7. `generate_structured()` implementation with retry logic:
   - Exponential backoff: wait times [2s, 4s, 8s] for 3 retries
   - Retry on: `RateLimitError`, `APIConnectionError`, `APITimeoutError`
   - No retry on: `AuthenticationError`, `InvalidRequestError`, `ValidationError`
   - Fail-fast on Pydantic ValidationError (LLM returned invalid schema)
   - Log each retry attempt with context

### Language Support
8. Hungarian/English language support:
   - System prompt includes: "Respond in Hungarian" or "Respond in English" based on `response_language`
   - Language detection: If query contains English words >50%, respond in English

### Cost Estimation
9. Cost estimation implementation:
   - GPT-4o-mini: $0.00015 per 1K input tokens, $0.0006 per 1K output tokens
   - GPT-4: $0.03 per 1K input tokens, $0.06 per 1K output tokens
   - Claude 3.5 Sonnet: $0.003 per 1K input tokens, $0.015 per 1K output tokens

### Multi-LLM Router
10. `MultiLLMRouter` class in `src/llm/llm_router.py`:
    - Provider registry: Maps provider name → LLMProvider instance
    - `async def generate(prompt: str, provider: str, options: GenerationOptions) -> LLMResponse`
    - `async def generate_structured(prompt: str, provider: str, options: GenerationOptions, response_schema: type[BaseModel]) -> LLMStructuredResponse`
    - Provider selection: Use specified provider or default from `LLM_DEFAULT_PROVIDER` env
    - No fallback logic: Fail fast if provider fails

### Observability
11. Structured logging:
    - Provider name, model, tokens, cost, latency
    - Structured output validation success/failure
    - Server-side vs client-side validation path taken
    - Retry attempts with errors
    - Final success/failure status

### Testing
12. Unit tests covering:
    - Pydantic model validation (smalltalk=true/false cases)
    - OpenAI provider: Successful structured generation (beta.chat.completions.parse)
    - OpenAI provider: Client-side fallback (response_format + manual validation)
    - Anthropic provider: Successful structured generation (beta.messages.parse)
    - Anthropic provider: Client-side fallback (tool use pattern)
    - Retry logic: Retryable errors (3 retries, then fail)
    - Retry logic: Non-retryable errors (fail immediately, including ValidationError)
    - Cost estimation accuracy
    - Language support (Hungarian and English prompts)
    - Multi-LLM router: Provider selection and routing
    - Multi-LLM router: Unknown provider error
    - Graceful degradation from server-side to client-side validation

## Tasks / Subtasks

- [ ] Task 1: Add Pydantic dependency (AC: Dependencies)
  - [ ] Add `pydantic = "^2.0"` to `pyproject.toml`
  - [ ] Run `poetry lock && poetry install`
  - [ ] Verify Pydantic v2 installed

- [ ] Task 2: Create structured output Pydantic model (AC: 1)
  - [ ] Create `src/llm/` module with `__init__.py`
  - [ ] Create `src/llm/structured_output.py`
  - [ ] Define `LLMStructuredResponse` with fields
  - [ ] Add Pydantic validators for smalltalk-based field requirements
  - [ ] Add JSON schema generation method
  - [ ] Test model validation with various inputs

- [ ] Task 3: Create base provider and data classes (AC: 2, 3, 4)
  - [ ] Create `src/llm/base_provider.py`
  - [ ] Define `LLMProvider` abstract base class
  - [ ] Define `GenerationOptions` dataclass
  - [ ] Define `LLMResponse` dataclass
  - [ ] Add abstract methods with type hints

- [ ] Task 4: Implement OpenAI provider (AC: 5, 7, 9)
  - [ ] Create `src/llm/providers/` module with `__init__.py`
  - [ ] Create `src/llm/providers/openai_provider.py`
  - [ ] Implement `OpenAIProvider` class
  - [ ] Implement `generate()` method (text-only)
  - [ ] Implement `generate_structured()` with beta.chat.completions.parse
  - [ ] Implement client-side fallback validation
  - [ ] Implement retry logic with exponential backoff
  - [ ] Implement cost estimation
  - [ ] Add structured logging
  - [ ] Load API key and model from environment

- [ ] Task 5: Implement Anthropic provider (AC: 6, 7, 9)
  - [ ] Create `src/llm/providers/anthropic_provider.py`
  - [ ] Implement `AnthropicProvider` class
  - [ ] Implement `generate()` method (text-only)
  - [ ] Implement `generate_structured()` with beta.messages.parse
  - [ ] Add `anthropic-beta: structured-outputs-2025-11-13` header
  - [ ] Implement tool use fallback validation
  - [ ] Implement retry logic with exponential backoff
  - [ ] Implement cost estimation
  - [ ] Add structured logging
  - [ ] Load API key and model from environment

- [ ] Task 6: Implement Multi-LLM Router (AC: 10)
  - [ ] Create `src/llm/llm_router.py`
  - [ ] Implement `MultiLLMRouter` class
  - [ ] Create provider registry
  - [ ] Implement `generate()` method with routing
  - [ ] Implement `generate_structured()` method with routing
  - [ ] Add provider selection logic
  - [ ] Add error handling for unknown providers
  - [ ] Load default provider from environment

- [ ] Task 7: Add language support (AC: 8)
  - [ ] Add language parameter to GenerationOptions
  - [ ] Implement language detection helper
  - [ ] Add language instruction to system prompts
  - [ ] Test Hungarian and English responses

- [ ] Task 8: Add environment configuration
  - [ ] Document all env vars in `.env.example`:
    - `OPENAI_API_KEY`
    - `OPENAI_QUERY_MODEL`
    - `ANTHROPIC_API_KEY`
    - `ANTHROPIC_QUERY_MODEL`
    - `LLM_DEFAULT_PROVIDER`

- [ ] Task 9: Write unit tests (AC: 12)
  - [ ] Test LLMStructuredResponse validation (smalltalk cases)
  - [ ] Test OpenAI provider server-side structured output
  - [ ] Test OpenAI provider client-side fallback
  - [ ] Test Anthropic provider server-side structured output
  - [ ] Test Anthropic provider tool use fallback
  - [ ] Test retry logic for retryable errors
  - [ ] Test retry logic for non-retryable errors
  - [ ] Test cost estimation accuracy
  - [ ] Test language support (Hungarian/English)
  - [ ] Test Multi-LLM router provider selection
  - [ ] Test Multi-LLM router unknown provider error
  - [ ] Mock OpenAI and Anthropic SDKs

- [ ] Task 10: Integration testing
  - [ ] Test OpenAI provider with real API (mark as integration test)
  - [ ] Test Anthropic provider with real API (mark as integration test)
  - [ ] Verify structured output validation end-to-end
  - [ ] Verify retry logic with simulated failures

## Dev Notes

### Technical Background
- **Strategy Pattern:** Enables future provider extensibility (Gemini, Grok, Mistral)
- **Pydantic Scope:** ONLY for LLM responses; existing dataclasses remain unchanged
- **Server-Side Benefits:** Reduces latency and improves reliability
- **Anthropic Advantage:** Constrained decoding is most reliable (grammar guarantees)
- **OpenAI Limitation:** beta.chat.completions.parse functional but has occasional validation errors
- **Fail-Fast Philosophy:** No automatic fallbacks to prevent unexpected behavior
- **Hungarian Priority:** Critical for MVP user base
- **Retry Strategy:** Essential for production reliability (handles transient API failures)

### Pydantic Model Example
```python
from pydantic import BaseModel, HttpUrl, field_validator

class LLMStructuredResponse(BaseModel):
    """Structured LLM response with validation."""
    answer: str | None = None
    personality_reply: str
    sources: list[HttpUrl] | None = None
    smalltalk: bool

    @field_validator('answer')
    def answer_required_for_lore(cls, v, info):
        if not info.data.get('smalltalk') and not v:
            raise ValueError('answer required when smalltalk=false')
        return v

    @field_validator('sources')
    def sources_required_for_lore(cls, v, info):
        if not info.data.get('smalltalk') and not v:
            raise ValueError('sources required when smalltalk=false')
        return v
```

### OpenAI Structured Output Pattern
```python
async def generate_structured(
    self,
    prompt: str,
    options: GenerationOptions,
    response_schema: type[BaseModel]
) -> LLMStructuredResponse:
    """Generate structured output using OpenAI."""
    try:
        # Primary: Server-side validation
        completion = await self.client.beta.chat.completions.parse(
            model=options.model,
            messages=[{"role": "user", "content": prompt}],
            response_format=response_schema
        )
        return completion.choices[0].message.parsed
    except Exception as e:
        logger.warning("beta_api_failed", error=str(e))
        # Fallback: Client-side validation
        completion = await self.client.chat.completions.create(
            model=options.model,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_schema", "schema": response_schema.model_json_schema()}
        )
        json_response = json.loads(completion.choices[0].message.content)
        return response_schema.model_validate(json_response)
```

### Anthropic Structured Output Pattern
```python
async def generate_structured(
    self,
    prompt: str,
    options: GenerationOptions,
    response_schema: type[BaseModel]
) -> LLMStructuredResponse:
    """Generate structured output using Anthropic."""
    try:
        # Primary: Server-side constrained decoding
        message = await self.client.beta.messages.parse(
            model=options.model,
            max_tokens=options.max_tokens,
            messages=[{"role": "user", "content": prompt}],
            response_model=response_schema,
            extra_headers={"anthropic-beta": "structured-outputs-2025-11-13"}
        )
        return message.parsed
    except Exception as e:
        logger.warning("beta_api_failed", error=str(e))
        # Fallback: Tool use pattern
        # ... (tool use implementation)
```

### Retry Logic Pattern
```python
async def _retry_with_backoff(
    self,
    func: Callable,
    *args,
    **kwargs
) -> Any:
    """Retry with exponential backoff."""
    wait_times = [2, 4, 8]  # seconds

    for attempt, wait in enumerate(wait_times, start=1):
        try:
            return await func(*args, **kwargs)
        except (RateLimitError, APIConnectionError, APITimeoutError) as e:
            if attempt == len(wait_times):
                raise
            logger.warning("retrying_llm_call", attempt=attempt, wait=wait, error=str(e))
            await asyncio.sleep(wait)
        except (AuthenticationError, InvalidRequestError, ValidationError):
            # Fail fast on non-retryable errors
            raise
```

### Environment Configuration
Add to `.env.example`:
```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-...
OPENAI_QUERY_MODEL=gpt-4o-mini           # Options: gpt-4o-mini, gpt-4

# Anthropic Configuration
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_QUERY_MODEL=claude-3-5-sonnet-20241022

# LLM Router Configuration
LLM_DEFAULT_PROVIDER=openai              # Options: openai, anthropic
```

### Relevant Source Tree
```
src/llm/
├── __init__.py                          # NEW: LLM module exports
├── structured_output.py                 # NEW: Pydantic models
├── base_provider.py                     # NEW: Abstract base class
├── llm_router.py                        # NEW: MultiLLMRouter
├── providers/
│   ├── __init__.py                      # NEW: Provider exports
│   ├── openai_provider.py               # NEW: OpenAIProvider
│   └── anthropic_provider.py            # NEW: AnthropicProvider
tests/unit/
├── test_structured_output.py            # NEW: Pydantic model tests
├── test_openai_provider.py              # NEW: OpenAI tests
├── test_anthropic_provider.py           # NEW: Anthropic tests
├── test_llm_router.py                   # NEW: Router tests
tests/integration/
├── test_llm_providers.py                # NEW: Real API tests (integration)
```

### Testing Strategy
- **Unit Tests:** Mock OpenAI and Anthropic SDKs
- **Integration Tests:** Use real APIs (mark with `@pytest.mark.integration`)
- **Cost Awareness:** Integration tests cost real money (use minimal tokens)
- **Validation Testing:** Test both valid and invalid structured outputs

### Cost Estimation Reference
| Provider | Model | Input (per 1K) | Output (per 1K) |
|----------|-------|----------------|-----------------|
| OpenAI | gpt-4o-mini | $0.00015 | $0.0006 |
| OpenAI | gpt-4 | $0.03 | $0.06 |
| Anthropic | claude-3-5-sonnet-20241022 | $0.003 | $0.015 |

Average query: ~500 input tokens, ~200 output tokens
- GPT-4o-mini: ~$0.0002 per query
- Claude 3.5 Sonnet: ~$0.0045 per query

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-05 | 1.0 | Story created from Epic 2 | John (PM Agent) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Pull Request
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_

---

**Estimated Effort:** 4 hours
