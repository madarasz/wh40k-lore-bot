# Story 1.4: Wiki XML Parser Implementation

## Status
Draft

## Story
**As a** developer,
**I want** to parse the WH40K Fandom Wiki XML export and convert articles to markdown,
**so that** I have clean, structured content for chunking and embedding.

## Acceptance Criteria
1. `src/ingestion/wiki_xml_parser.py` created with `WikiXMLParser` class
2. Parser uses lxml's iterparse for memory-efficient streaming of large XML files
3. **CRITICAL: Namespace filtering - Only process pages with `<ns>0</ns>` (main articles)**
4. XML namespace handling: `{http://www.mediawiki.org/xml/export-0.11/}`
5. `parse_xml_export(xml_path: str, page_ids: Optional[List[str]] = None)` method extracts article elements using XPath
6. **Page ID filtering:** If `page_ids` list provided, only process pages whose `<id>` is in the list
7. Each article extracts: **wiki page id**, title, timestamp, content (MediaWiki markup)
8. **MediaWiki markup converted to Markdown using mwparserfromhell library:**
   - Headings: `==Text==` → `## Text`, `===Text===` → `### Text`, `====Text====` → `#### Text`
   - Bold: `'''text'''` → `**text**`
   - Italic: `''text''` → `*text*`
   - Internal links: `[[Link]]` → `[Link](Link)` and `[[Link|text]]` → `[text](Link)`
   - External links: `[http://example.com text]` → `[text](http://example.com)`
   - Lists: `* item` → `- item`, `# item` → `1. item`
   - Strip or simplify templates (e.g., `{{Quote|...}}` → plain text or removed)
   - Remove File/Image embeds: `[[File:...]]` removed or converted to `[Image: filename]`
   - Remove Categories: `[[Category:...]]` removed
9. Extract internal links from wikitext for metadata `links` array
10. `save_articles_batch()` writes markdown files to `data/markdown-archive/{sanitized_title}.md`
11. **Sanitize filenames** (handle spaces, slashes, colons, special characters)
12. Parser handles malformed XML gracefully with defusedxml security protection
13. **Memory cleanup: `elem.clear()` after processing each page**
14. Progress logging every 100 articles parsed (e.g., "Parsed 500/10000 articles...")
15. Integration test with full 173MB XML file verifies end-to-end parsing
16. Parses 173MB XML file in <500MB memory footprint
17. CLI command supports: `poetry run parse-wiki data/warhammer40k_pages_current.xml [--page-ids-file pages.txt]`

## Tasks / Subtasks

- [ ] Create WikiXMLParser class (AC: 1, 2, 4)
  - [ ] Create src/ingestion/wiki_xml_parser.py
  - [ ] Implement WikiXMLParser class
  - [ ] Define namespace constant: NS = '{http://www.mediawiki.org/xml/export-0.11/}'
  - [ ] Use lxml.etree.iterparse() for streaming
  - [ ] Add structlog logger instance
  - [ ] Add type hints and docstring
- [ ] Implement XML parsing with namespace filtering (AC: 3, 5, 6, 7)
  - [ ] Implement parse_xml_export(xml_path, page_ids=None) method
  - [ ] Use iterparse with events=('end',) for element completion
  - [ ] Filter for pages with <ns>0</ns> (main namespace only)
  - [ ] Extract wiki page id from <id> tag
  - [ ] Implement page_ids filtering if provided
  - [ ] Extract title from <title> tag
  - [ ] Extract timestamp from <timestamp> tag
  - [ ] Extract wikitext from <revision><text> tag
  - [ ] Call elem.clear() after processing each page for memory cleanup
- [ ] Implement MediaWiki to Markdown conversion (AC: 8)
  - [ ] Create convert_wikitext_to_markdown(wikitext: str) -> str function
  - [ ] Use mwparserfromhell.parse() to parse wikitext
  - [ ] Convert headings (==, ===, ====) to markdown (#, ##, ###)
  - [ ] Convert bold (''') to **text**
  - [ ] Convert italic ('') to *text*
  - [ ] Convert internal links [[Link]] and [[Link|text]]
  - [ ] Convert external links [url text]
  - [ ] Convert lists (* and #)
  - [ ] Strip or simplify templates {{...}}
  - [ ] Remove File/Image embeds [[File:...]]
  - [ ] Remove Category links [[Category:...]]
  - [ ] Handle nested formatting edge cases
- [ ] Implement link extraction for metadata (AC: 9)
  - [ ] Create extract_internal_links(wikitext: str) -> List[str] function
  - [ ] Parse wikitext with mwparserfromhell
  - [ ] Extract all [[...]] internal link targets
  - [ ] Handle [[Link|Display]] format (extract Link only)
  - [ ] Return deduplicated list of link targets
- [ ] Implement batch saving to markdown archive (AC: 10, 11)
  - [ ] Implement save_articles_batch(articles: List[WikiArticle]) method
  - [ ] Use save_markdown_file from Story 1.3
  - [ ] Use sanitize_filename from Story 1.3
  - [ ] Handle file write errors gracefully
  - [ ] Log successful saves
- [ ] Add security and error handling (AC: 12, 13)
  - [ ] Wrap XML parsing with defusedxml for security
  - [ ] Handle malformed XML with try/except
  - [ ] Log parsing errors with article context
  - [ ] Continue processing on individual article failures
  - [ ] Ensure elem.clear() is always called (use try/finally)
- [ ] Add progress logging (AC: 14)
  - [ ] Track article count processed
  - [ ] Log progress every 100 articles
  - [ ] Log format: "Parsed {count}/{total} articles..."
  - [ ] Log final summary stats
- [ ] Create CLI command (AC: 17)
  - [ ] Create src/cli/parse_wiki.py
  - [ ] Use click for CLI framework
  - [ ] Add xml_path argument
  - [ ] Add --page-ids-file option
  - [ ] Load page IDs from file if provided (one per line)
  - [ ] Display progress to console
  - [ ] Add to pyproject.toml [tool.poetry.scripts]
- [ ] Write unit tests for MediaWiki conversion
  - [ ] Create tests/unit/test_wiki_xml_parser.py
  - [ ] Test heading conversion (all levels)
  - [ ] Test bold and italic conversion
  - [ ] Test internal link conversion (both formats)
  - [ ] Test external link conversion
  - [ ] Test list conversion (unordered and ordered)
  - [ ] Test template stripping
  - [ ] Test File/Image removal
  - [ ] Test Category removal
  - [ ] Test complex nested formatting
- [ ] Write unit tests for link extraction
  - [ ] Test simple [[Link]] extraction
  - [ ] Test [[Link|Display]] extraction
  - [ ] Test multiple links in text
  - [ ] Test deduplication
- [ ] Write integration test with sample XML (AC: 15)
  - [ ] Create tests/integration/test_xml_parsing.py
  - [ ] Create small sample XML with 5-10 test pages
  - [ ] Include pages with ns=0 and ns≠0 (verify filtering)
  - [ ] Test end-to-end parsing and markdown conversion
  - [ ] Verify markdown files created correctly
- [ ] Performance testing (AC: 16)
  - [ ] Test with full 173MB XML file
  - [ ] Monitor memory usage during parsing
  - [ ] Verify <500MB memory footprint
  - [ ] Log processing time
  - [ ] Optimize if needed
- [ ] Verify all acceptance criteria met
  - [ ] Run all tests with `poetry run pytest`
  - [ ] Test CLI command with sample XML
  - [ ] Test with --page-ids-file option
  - [ ] Verify namespace filtering works
  - [ ] Verify memory cleanup works

## Dev Notes

### Previous Story Insights
Story 1.3 completed:
- Markdown archive directory structure created
- Filename sanitization utility available
- save_markdown_file() function ready to use

### Technical Notes
**[Source: Epic 1 Story 1.4]**

- Use `lxml.etree.iterparse()` with `events=('end',)` for streaming
- MediaWiki namespace: `http://www.mediawiki.org/xml/export-0.11/`
- `mwparserfromhell` for robust MediaWiki markup parsing
- `defusedxml` to prevent XML bomb attacks
- Estimated processing time: 2-4 hours for full wiki
- Estimated article count: ~10,000-15,000 main articles

### Sample Code Pattern
```python
import xml.etree.ElementTree as ET
from defusedxml import ElementTree as DefusedET
import mwparserfromhell

namespaces = {'mw': 'http://www.mediawiki.org/xml/export-0.11/'}

for event, elem in ET.iterparse(xml_path, events=('end',)):
    if elem.tag == '{http://www.mediawiki.org/xml/export-0.11/}page':
        ns_elem = elem.find('mw:ns', namespaces)
        if ns_elem is not None and ns_elem.text == '0':
            # Process main article
            title = elem.find('mw:title', namespaces).text
            text = elem.find('.//mw:revision/mw:text', namespaces).text

            # Convert MediaWiki to Markdown
            wikicode = mwparserfromhell.parse(text)
            markdown = convert_to_markdown(wikicode)

        # Free memory
        elem.clear()
```

### MediaWiki to Markdown Conversion Rules

**Headings:**
- `==Text==` → `## Text`
- `===Text===` → `### Text`
- `====Text====` → `#### Text`

**Formatting:**
- `'''bold'''` → `**bold**`
- `''italic''` → `*italic*`

**Links:**
- `[[Link]]` → `[Link](Link)`
- `[[Link|Display Text]]` → `[Display Text](Link)`
- `[http://example.com Text]` → `[Text](http://example.com)`

**Lists:**
- `* item` → `- item`
- `# item` → `1. item`

**Remove:**
- `[[File:...]]` - Image embeds
- `[[Category:...]]` - Category tags
- `{{Template|...}}` - Templates (or simplify to plain text)

### XML Structure Reference

```xml
<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.11/">
  <page>
    <id>58</id>
    <ns>0</ns>
    <title>Blood Angels</title>
    <revision>
      <timestamp>2024-09-20T14:18:31Z</timestamp>
      <text>...MediaWiki markup...</text>
    </revision>
  </page>
</mediawiki>
```

### Coding Standards Reminders

**[Source: architecture/coding-standards.md]**
- Type hints required for all function signatures
- Docstrings required for all public functions (Google style)
- Use structlog for logging with context
- Handle errors gracefully with appropriate exceptions
- Memory-efficient processing for large files

## Dev Agent Record

### Agent Model Used
[To be populated during implementation]

### Debug Log References
[To be populated during implementation]

### Completion Notes List
[To be populated during implementation]

### File List
[To be populated during implementation]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-26 | 1.0 | Story created from Epic 1 | Bob (Scrum Master) |

## QA Results
[To be populated by QA agent]
