# Story 1.6: Intelligent Text Chunking Implementation

## Status
Draft

## Story
**As a** developer,
**I want** to intelligently chunk markdown articles into semantically coherent pieces,
**so that** embeddings capture meaningful context without breaking mid-thought.

## Acceptance Criteria
1. `src/ingestion/text_chunker.py` created with `MarkdownChunker` class
2. Chunking strategy:
   - **Primary:** Chunk by section headers (##, ###, ####)
   - **Secondary:** If section > 500 tokens, split on paragraph boundaries
   - **Tertiary:** If paragraph > 500 tokens, split on sentence boundaries
   - **Max chunk size:** 500 tokens (optimal for embeddings)
   - **Min chunk size:** 50 tokens (avoid tiny fragments)
3. Section hierarchy preserved in metadata:
   - `section_path`: "History > Horus Heresy > Battle of Terra"
4. `chunk_markdown(markdown: str, article_title: str) -> List[Chunk]` method
5. Each chunk includes:
   - `chunk_text`: The actual text content
   - `article_title`: Source article
   - `section_path`: Hierarchical section path
   - `chunk_index`: 0-based index within article
6. Tokenizer uses `tiktoken` with `cl100k_base` encoding (OpenAI)
7. Unit tests with sample markdown covering:
   - Single section (no splitting)
   - Multiple sections
   - Long section requiring paragraph split
   - Very long paragraph requiring sentence split
8. Integration test with real wiki article (e.g., "Tyranids" - 72KB)

## Tasks / Subtasks

- [ ] Create MarkdownChunker class (AC: 1)
  - [ ] Create src/ingestion/text_chunker.py
  - [ ] Implement MarkdownChunker class
  - [ ] Add tiktoken tokenizer (cl100k_base encoding)
  - [ ] Define constants: MAX_TOKENS=500, MIN_TOKENS=50
  - [ ] Add structlog logger instance
  - [ ] Add type hints and docstring
- [ ] Create Chunk dataclass
  - [ ] Create src/ingestion/models.py if not exists
  - [ ] Define Chunk dataclass with fields:
  - [ ] chunk_text: str
  - [ ] article_title: str
  - [ ] section_path: str
  - [ ] chunk_index: int
  - [ ] Add type hints
- [ ] Implement markdown section parsing (AC: 2, 3)
  - [ ] Implement _parse_sections(markdown: str) -> List[Section]
  - [ ] Parse markdown by headers (##, ###, ####)
  - [ ] Build section hierarchy tree
  - [ ] Generate section_path strings ("Parent > Child > Grandchild")
  - [ ] Handle header level changes correctly
  - [ ] Return list of Section objects with text and path
- [ ] Implement token counting (AC: 6)
  - [ ] Implement _count_tokens(text: str) -> int
  - [ ] Use tiktoken with cl100k_base encoding
  - [ ] Cache encoder instance for performance
  - [ ] Handle empty strings gracefully
- [ ] Implement chunking strategy (AC: 2, 4, 5)
  - [ ] Implement chunk_markdown(markdown, article_title) -> List[Chunk]
  - [ ] Primary: Split by section headers
  - [ ] For each section:
  - [ ] If section <= 500 tokens: keep as single chunk
  - [ ] If section > 500 tokens: split by paragraphs
  - [ ] If paragraph > 500 tokens: split by sentences
  - [ ] Ensure min chunk size (50 tokens)
  - [ ] Merge tiny chunks with adjacent chunks
  - [ ] Preserve section headers in chunks
  - [ ] Assign chunk_index (0-based)
- [ ] Implement paragraph splitting
  - [ ] Implement _split_by_paragraphs(text: str) -> List[str]
  - [ ] Split on double newlines (\n\n)
  - [ ] Preserve markdown formatting
  - [ ] Handle edge cases (multiple newlines, trailing newlines)
- [ ] Implement sentence splitting
  - [ ] Implement _split_by_sentences(text: str) -> List[str]
  - [ ] Use regex for sentence boundaries (. ! ?)
  - [ ] Handle abbreviations (e.g., Dr., Mr., vs.)
  - [ ] Handle decimal numbers (don't split on decimal point)
  - [ ] Preserve markdown formatting
- [ ] Handle edge cases
  - [ ] Empty sections → skip
  - [ ] Sections with only headers → skip
  - [ ] Very short articles (<50 tokens) → single chunk
  - [ ] Articles with no headers → treat as single section
  - [ ] Nested lists and code blocks → preserve structure
- [ ] Write unit tests (AC: 7)
  - [ ] Create tests/unit/test_text_chunker.py
  - [ ] Test single section (no splitting)
  - [ ] Test multiple sections
  - [ ] Test long section requiring paragraph split
  - [ ] Test very long paragraph requiring sentence split
  - [ ] Test section hierarchy preservation
  - [ ] Test chunk_index assignment
  - [ ] Test min/max token constraints
  - [ ] Test edge cases (empty, no headers, etc.)
- [ ] Write integration test (AC: 8)
  - [ ] Create tests/integration/test_chunking.py
  - [ ] Test with real "Tyranids" article (~72KB)
  - [ ] Verify all chunks within 50-500 token range
  - [ ] Verify section paths are correct
  - [ ] Verify no content lost
  - [ ] Log chunking statistics (count, avg size, etc.)
- [ ] Verify all acceptance criteria met
  - [ ] Run all tests with `poetry run pytest`
  - [ ] Test with various markdown samples
  - [ ] Verify token counting accuracy
  - [ ] Verify chunk quality (no mid-sentence breaks)

## Dev Notes

### Previous Story Insights
Story 1.5 completed:
- Test bed of ~100 pages created
- Starting with Blood Angels as seed
- Fast dataset for testing chunking pipeline

### Technical Notes
**[Source: Epic 1 Story 1.6]**

- Use `tiktoken` for accurate token counting (matches OpenAI)
- Preserve markdown formatting in chunks (headers, lists, bold)
- Overlap strategy: Include section header in every chunk from that section

### Chunking Strategy Hierarchy

```
1. PRIMARY: Split by section headers
   ├─ ## Section → Chunk boundary
   ├─ ### Subsection → Chunk boundary
   └─ #### Sub-subsection → Chunk boundary

2. SECONDARY: If section > 500 tokens
   └─ Split by paragraphs (\\n\\n)

3. TERTIARY: If paragraph > 500 tokens
   └─ Split by sentences (. ! ?)

4. POST-PROCESSING:
   ├─ Merge chunks < 50 tokens
   └─ Ensure chunks <= 500 tokens
```

### Section Path Examples

```markdown
## History
### The Great Crusade
#### Unification Wars

→ section_path: "History > The Great Crusade > Unification Wars"
```

### Token Counting

```python
import tiktoken

# Initialize encoder (cache this!)
encoder = tiktoken.get_encoding("cl100k_base")

# Count tokens
tokens = encoder.encode(text)
token_count = len(tokens)
```

**Token estimates:**
- 1 token ≈ 4 characters (English)
- 500 tokens ≈ 2000 characters ≈ 300-400 words

### Sentence Splitting Edge Cases

**Handle carefully:**
- Abbreviations: "Dr. Smith", "Mr. Jones", "vs."
- Decimals: "3.14", "99.9%"
- Ellipsis: "..."
- Quotes: "said. \"Hello\""

**Regex pattern:**
```python
# Simple sentence boundary
sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)

# More robust (handle abbreviations)
sentences = re.split(r'(?<=[.!?])(?<![A-Z]\.)\s+(?=[A-Z])', text)
```

### Coding Standards Reminders

**[Source: architecture/coding-standards.md]**
- Type hints required for all function signatures
- Docstrings required for all public functions (Google style)
- Use dataclasses for structured data (Chunk, Section)
- Cache expensive operations (tiktoken encoder)
- Log chunk statistics for monitoring

## Dev Agent Record

### Agent Model Used
[To be populated during implementation]

### Debug Log References
[To be populated during implementation]

### Completion Notes List
[To be populated during implementation]

### File List
[To be populated during implementation]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-26 | 1.0 | Story created from Epic 1 | Bob (Scrum Master) |

## QA Results
[To be populated by QA agent]
