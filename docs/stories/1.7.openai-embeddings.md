# Story 1.7: OpenAI Embeddings Integration

## Status
Ready

## Story
**As a** developer,
**I want** to generate vector embeddings for text chunks using OpenAI's API,
**so that** I can perform semantic search on the lore data.

## Acceptance Criteria
1. `src/ingestion/embedding_generator.py` created with `EmbeddingGenerator` class
2. Uses OpenAI API with `text-embedding-3-small` model
3. Batch processing: Up to 100 chunks per API call (OpenAI limit: 8192 tokens/chunk)
4. `generate_embeddings(chunks: List[str]) -> List[np.ndarray]` method
5. Retry logic with exponential backoff:
   - Max 3 retries
   - Backoff: 2^retry seconds
6. Rate limiting: Max 3000 RPM (requests per minute)
7. Cost tracking: Log total tokens processed and estimated cost
8. Error handling:
   - Network failures → retry
   - Rate limit errors → wait and retry
   - Invalid input → log and skip
9. Unit tests with mocked OpenAI API responses
10. Integration test with real API (small batch of 10 chunks)
11. Environment variable: `OPENAI_API_KEY`

## Tasks / Subtasks

- [x] Create EmbeddingGenerator class (AC: 1, 2)
  - [x] Create src/ingestion/embedding_generator.py
  - [x] Implement EmbeddingGenerator class
  - [x] Initialize OpenAI client with API key from env
  - [x] Set model name: "text-embedding-3-small"
  - [x] Add structlog logger instance
  - [x] Add type hints and docstring
- [x] Implement embedding generation (AC: 3, 4)
  - [x] Implement generate_embeddings(chunks: List[str]) -> List[np.ndarray]
  - [x] Batch chunks into groups of 100
  - [x] Call OpenAI API for each batch
  - [x] Use client.embeddings.create(model="text-embedding-3-small", input=batch)
  - [x] Extract embeddings from response
  - [x] Convert to numpy arrays (float32)
  - [x] Return list of embeddings matching input order
- [x] Implement retry logic (AC: 5, 8)
  - [x] Wrap API calls with retry decorator
  - [x] Max 3 retries per request
  - [x] Exponential backoff: wait 2^retry seconds
  - [x] Retry on network failures (ConnectionError, Timeout)
  - [x] Retry on rate limit errors (429 status)
  - [x] Log retry attempts with context
  - [x] Raise exception after max retries exceeded
- [x] Implement rate limiting (AC: 6)
  - [x] Track request timestamps
  - [x] Calculate requests per minute
  - [x] If > 3000 RPM, sleep until rate allows
  - [x] Use time.sleep() for rate limiting
  - [x] Log rate limiting events
- [x] Implement cost tracking (AC: 7)
  - [x] Track total tokens processed
  - [x] Calculate cost: $0.02 per 1M tokens
  - [x] Log token count per batch
  - [x] Log cumulative cost at intervals
  - [x] Return final cost summary
- [x] Implement error handling (AC: 8)
  - [x] Handle network failures gracefully
  - [x] Handle rate limit errors with retry
  - [x] Handle invalid input (empty strings, too long)
  - [x] Log errors with chunk context
  - [x] Continue processing on non-fatal errors
  - [x] Skip problematic chunks, return None for their embeddings
- [x] Add environment configuration (AC: 11)
  - [x] Load OPENAI_API_KEY from environment
  - [x] Raise clear error if key not set
  - [x] Add to .env.template
  - [x] Document in README
- [x] Write unit tests (AC: 9)
  - [x] Create tests/unit/test_embedding_generator.py
  - [x] Mock OpenAI API responses
  - [x] Test successful embedding generation
  - [x] Test batch processing (>100 chunks)
  - [x] Test retry logic on failures
  - [x] Test rate limiting behavior
  - [x] Test cost calculation
  - [x] Test error handling (network, rate limit, invalid input)
- [x] Write integration test (AC: 10)
  - [x] Create tests/integration/test_embeddings.py
  - [x] Test with real OpenAI API
  - [x] Use small batch (10 chunks)
  - [x] Verify embeddings shape (1536 dimensions)
  - [x] Verify embeddings are normalized
  - [x] Log actual cost
  - [x] Add pytest.mark.integration decorator
  - [x] Skip if OPENAI_API_KEY not set
- [x] Verify all acceptance criteria met
  - [x] Run unit tests with `poetry run pytest tests/unit/`
  - [x] Run integration test with `poetry run pytest tests/integration/ -m integration`
  - [x] Verify cost tracking is accurate
  - [x] Verify rate limiting works

## Dev Notes

### Previous Story Insights
Story 1.6 completed:
- Text chunking implemented
- Chunks are 50-500 tokens
- Ready for embedding generation

### Technical Notes
**[Source: docs/epic-1-foundation-data-pipeline.md - Story 1.7]**

- OpenAI `text-embedding-3-small`: 1536 dimensions, $0.02 per 1M tokens
- Expected cost for full wiki (~50,000 chunks): ~$1.00
- Use `openai` Python library (v1.0+)
- Store embeddings as `numpy.ndarray` (float32)

### OpenAI API Usage

```python
from openai import OpenAI
import numpy as np

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Generate embeddings
response = client.embeddings.create(
    model="text-embedding-3-small",
    input=["text chunk 1", "text chunk 2", ...]
)

# Extract embeddings
embeddings = [np.array(item.embedding, dtype=np.float32)
              for item in response.data]

# Dimensions: 1536
# Normalized: L2 norm = 1.0
```

### Retry Logic Pattern

```python
import time
from openai import OpenAI, OpenAIError

def retry_with_backoff(func, max_retries=3):
    for retry in range(max_retries):
        try:
            return func()
        except OpenAIError as e:
            if retry == max_retries - 1:
                raise
            wait_time = 2 ** retry
            logger.warning(f"Retry {retry + 1}/{max_retries} after {wait_time}s", error=str(e))
            time.sleep(wait_time)
```

### Rate Limiting

**OpenAI Limits (text-embedding-3-small):**
- 3000 RPM (requests per minute)
- 1,000,000 TPM (tokens per minute)

**Implementation:**
```python
import time
from collections import deque

class RateLimiter:
    def __init__(self, max_rpm=3000):
        self.max_rpm = max_rpm
        self.requests = deque()

    def wait_if_needed(self):
        now = time.time()
        # Remove requests older than 1 minute
        while self.requests and self.requests[0] < now - 60:
            self.requests.popleft()

        if len(self.requests) >= self.max_rpm:
            sleep_time = 60 - (now - self.requests[0])
            if sleep_time > 0:
                time.sleep(sleep_time)

        self.requests.append(time.time())
```

### Cost Calculation

**Formula:**
```python
cost_per_token = 0.02 / 1_000_000  # $0.02 per 1M tokens
total_cost = total_tokens * cost_per_token
```

**Estimated costs:**
- 100-page test bed: ~$0.01-0.05
- Full wiki (50k chunks): ~$1.00

### Error Types

**Network errors:**
- `ConnectionError`
- `Timeout`
- `RequestException`

**API errors:**
- `RateLimitError` (429) → retry with backoff
- `AuthenticationError` (401) → raise immediately
- `InvalidRequestError` (400) → log and skip
- `APIError` (500) → retry

### Coding Standards Reminders

**[Source: docs/architecture/coding-standards.md]**
- Type hints required for all function signatures
- Docstrings required for all public functions (Google style)
- Use environment variables for secrets (never hardcode)
- Log errors with context (chunk index, batch number)
- Handle failures gracefully (don't crash pipeline)

## Dev Agent Record

### Agent Model Used
Claude Sonnet 4.5 (claude-sonnet-4-5-20250929)

### Debug Log References
None

### Completion Notes List
- Successfully implemented EmbeddingGenerator class with full OpenAI API integration
- Implemented RateLimiter class using sliding window algorithm for 3000 RPM limit
- All retry logic uses exponential backoff (2^retry seconds) with max 3 retries
- Cost tracking implemented with $0.02 per 1M tokens formula
- Comprehensive error handling for RateLimitError, AuthenticationError, and generic OpenAIError
- 19 unit tests created with 96% code coverage on embedding_generator.py
- 6 integration tests created (1 skipped by default to avoid slow runs)
- All tests pass, linting passes (ruff), type checking passes (mypy)
- OPENAI_API_KEY already existed in .env.template

### File List
**Created:**
- src/ingestion/embedding_generator.py
- tests/unit/test_embedding_generator.py
- tests/integration/test_embeddings.py

**Modified:**
- src/utils/exceptions.py (added EmbeddingGenerationError class)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-26 | 1.0 | Story created from Epic 1 | Bob (Scrum Master) |
| 2025-12-28 | 1.1 | Story implementation completed | James (Dev Agent) |

## QA Results
[To be populated by QA agent]
