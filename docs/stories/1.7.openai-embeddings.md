# Story 1.7: OpenAI Embeddings Integration

## Status
Ready

## Story
**As a** developer,
**I want** to generate vector embeddings for text chunks using OpenAI's API,
**so that** I can perform semantic search on the lore data.

## Acceptance Criteria
1. `src/ingestion/embedding_generator.py` created with `EmbeddingGenerator` class
2. Uses OpenAI API with `text-embedding-3-small` model
3. Batch processing: Up to 100 chunks per API call (OpenAI limit: 8192 tokens/chunk)
4. `generate_embeddings(chunks: List[str]) -> List[np.ndarray]` method
5. Retry logic with exponential backoff:
   - Max 3 retries
   - Backoff: 2^retry seconds
6. Rate limiting: Max 3000 RPM (requests per minute)
7. Cost tracking: Log total tokens processed and estimated cost
8. Error handling:
   - Network failures → retry
   - Rate limit errors → wait and retry
   - Invalid input → log and skip
9. Unit tests with mocked OpenAI API responses
10. Integration test with real API (small batch of 10 chunks)
11. Environment variable: `OPENAI_API_KEY`

## Tasks / Subtasks

- [ ] Create EmbeddingGenerator class (AC: 1, 2)
  - [ ] Create src/ingestion/embedding_generator.py
  - [ ] Implement EmbeddingGenerator class
  - [ ] Initialize OpenAI client with API key from env
  - [ ] Set model name: "text-embedding-3-small"
  - [ ] Add structlog logger instance
  - [ ] Add type hints and docstring
- [ ] Implement embedding generation (AC: 3, 4)
  - [ ] Implement generate_embeddings(chunks: List[str]) -> List[np.ndarray]
  - [ ] Batch chunks into groups of 100
  - [ ] Call OpenAI API for each batch
  - [ ] Use client.embeddings.create(model="text-embedding-3-small", input=batch)
  - [ ] Extract embeddings from response
  - [ ] Convert to numpy arrays (float32)
  - [ ] Return list of embeddings matching input order
- [ ] Implement retry logic (AC: 5, 8)
  - [ ] Wrap API calls with retry decorator
  - [ ] Max 3 retries per request
  - [ ] Exponential backoff: wait 2^retry seconds
  - [ ] Retry on network failures (ConnectionError, Timeout)
  - [ ] Retry on rate limit errors (429 status)
  - [ ] Log retry attempts with context
  - [ ] Raise exception after max retries exceeded
- [ ] Implement rate limiting (AC: 6)
  - [ ] Track request timestamps
  - [ ] Calculate requests per minute
  - [ ] If > 3000 RPM, sleep until rate allows
  - [ ] Use time.sleep() for rate limiting
  - [ ] Log rate limiting events
- [ ] Implement cost tracking (AC: 7)
  - [ ] Track total tokens processed
  - [ ] Calculate cost: $0.02 per 1M tokens
  - [ ] Log token count per batch
  - [ ] Log cumulative cost at intervals
  - [ ] Return final cost summary
- [ ] Implement error handling (AC: 8)
  - [ ] Handle network failures gracefully
  - [ ] Handle rate limit errors with retry
  - [ ] Handle invalid input (empty strings, too long)
  - [ ] Log errors with chunk context
  - [ ] Continue processing on non-fatal errors
  - [ ] Skip problematic chunks, return None for their embeddings
- [ ] Add environment configuration (AC: 11)
  - [ ] Load OPENAI_API_KEY from environment
  - [ ] Raise clear error if key not set
  - [ ] Add to .env.template
  - [ ] Document in README
- [ ] Write unit tests (AC: 9)
  - [ ] Create tests/unit/test_embedding_generator.py
  - [ ] Mock OpenAI API responses
  - [ ] Test successful embedding generation
  - [ ] Test batch processing (>100 chunks)
  - [ ] Test retry logic on failures
  - [ ] Test rate limiting behavior
  - [ ] Test cost calculation
  - [ ] Test error handling (network, rate limit, invalid input)
- [ ] Write integration test (AC: 10)
  - [ ] Create tests/integration/test_embeddings.py
  - [ ] Test with real OpenAI API
  - [ ] Use small batch (10 chunks)
  - [ ] Verify embeddings shape (1536 dimensions)
  - [ ] Verify embeddings are normalized
  - [ ] Log actual cost
  - [ ] Add pytest.mark.integration decorator
  - [ ] Skip if OPENAI_API_KEY not set
- [ ] Verify all acceptance criteria met
  - [ ] Run unit tests with `poetry run pytest tests/unit/`
  - [ ] Run integration test with `poetry run pytest tests/integration/ -m integration`
  - [ ] Verify cost tracking is accurate
  - [ ] Verify rate limiting works

## Dev Notes

### Previous Story Insights
Story 1.6 completed:
- Text chunking implemented
- Chunks are 50-500 tokens
- Ready for embedding generation

### Technical Notes
**[Source: docs/epic-1-foundation-data-pipeline.md - Story 1.7]**

- OpenAI `text-embedding-3-small`: 1536 dimensions, $0.02 per 1M tokens
- Expected cost for full wiki (~50,000 chunks): ~$1.00
- Use `openai` Python library (v1.0+)
- Store embeddings as `numpy.ndarray` (float32)

### OpenAI API Usage

```python
from openai import OpenAI
import numpy as np

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Generate embeddings
response = client.embeddings.create(
    model="text-embedding-3-small",
    input=["text chunk 1", "text chunk 2", ...]
)

# Extract embeddings
embeddings = [np.array(item.embedding, dtype=np.float32)
              for item in response.data]

# Dimensions: 1536
# Normalized: L2 norm = 1.0
```

### Retry Logic Pattern

```python
import time
from openai import OpenAI, OpenAIError

def retry_with_backoff(func, max_retries=3):
    for retry in range(max_retries):
        try:
            return func()
        except OpenAIError as e:
            if retry == max_retries - 1:
                raise
            wait_time = 2 ** retry
            logger.warning(f"Retry {retry + 1}/{max_retries} after {wait_time}s", error=str(e))
            time.sleep(wait_time)
```

### Rate Limiting

**OpenAI Limits (text-embedding-3-small):**
- 3000 RPM (requests per minute)
- 1,000,000 TPM (tokens per minute)

**Implementation:**
```python
import time
from collections import deque

class RateLimiter:
    def __init__(self, max_rpm=3000):
        self.max_rpm = max_rpm
        self.requests = deque()

    def wait_if_needed(self):
        now = time.time()
        # Remove requests older than 1 minute
        while self.requests and self.requests[0] < now - 60:
            self.requests.popleft()

        if len(self.requests) >= self.max_rpm:
            sleep_time = 60 - (now - self.requests[0])
            if sleep_time > 0:
                time.sleep(sleep_time)

        self.requests.append(time.time())
```

### Cost Calculation

**Formula:**
```python
cost_per_token = 0.02 / 1_000_000  # $0.02 per 1M tokens
total_cost = total_tokens * cost_per_token
```

**Estimated costs:**
- 100-page test bed: ~$0.01-0.05
- Full wiki (50k chunks): ~$1.00

### Error Types

**Network errors:**
- `ConnectionError`
- `Timeout`
- `RequestException`

**API errors:**
- `RateLimitError` (429) → retry with backoff
- `AuthenticationError` (401) → raise immediately
- `InvalidRequestError` (400) → log and skip
- `APIError` (500) → retry

### Coding Standards Reminders

**[Source: docs/architecture/coding-standards.md]**
- Type hints required for all function signatures
- Docstrings required for all public functions (Google style)
- Use environment variables for secrets (never hardcode)
- Log errors with context (chunk index, batch number)
- Handle failures gracefully (don't crash pipeline)

## Dev Agent Record

### Agent Model Used
[To be populated during implementation]

### Debug Log References
[To be populated during implementation]

### Completion Notes List
[To be populated during implementation]

### File List
[To be populated during implementation]

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-12-26 | 1.0 | Story created from Epic 1 | Bob (Scrum Master) |

## QA Results
[To be populated by QA agent]
