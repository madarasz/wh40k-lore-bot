# Story 2.7: CLI Adapter with Structured Output Display

## Status
Not Started

## Epic
Epic 2: Core RAG Query System

## Dependencies
- Story 2.5: Query Orchestrator (`QueryOrchestrator`, `QueryRequest`, `QueryResponse`)
- Story 2.6: Query Logger (`QueryLogger`)
- Existing CLI infrastructure (typer, rich)
- Existing Chroma and BM25 repositories from Epic 1

## Story
**As a** developer,
**I want** a CLI command that displays structured LLM outputs (answer, personality, sources) with rich formatting,
**so that** I can test the RAG query system end-to-end and validate smalltalk handling before Discord integration.

## Acceptance Criteria

### CLIAdapter Class
1. `CLIAdapter` class created in `src/adapters/cli_adapter.py`:
   - Uses `typer` library for CLI framework
   - Command: `query <question> [--language] [--verbose]`
   - Example: `poetry run python -m src.adapters.cli_adapter query "Who is Roboute Guilliman?"`

### CLI Command Implementation
2. CLI command implementation:
   - `query` command:
     - Required argument: `question` (str)
     - Optional flags:
       - `--language` (choices: "hu", "en", default: "hu")
       - `--verbose` (flag, default: False)
   - Converts CLI args into `QueryRequest`
   - Calls `QueryOrchestrator.process(request)`
   - Displays structured output from `QueryResponse`
   - Prints metadata (latency, cost, smalltalk flag) if `--verbose` flag

### Rich Output Display
3. Structured output display (using `rich` library):
   - **For lore questions (smalltalk=false):**
     - Answer text in default color
     - Personality reply in italic dim style (visual distinction)
     - Sources section header in bold blue ("ðŸ“š ForrÃ¡sok:" or "ðŸ“š Sources:")
     - Source URLs as clickable links (if terminal supports)
     - Limit to 5 sources displayed
   - **For smalltalk (smalltalk=true):**
     - Only display personality_reply
     - No answer or sources section

### Error Handling
4. Error handling:
   - Catch all exceptions from orchestrator
   - Print user-friendly error messages in red
   - Exit with code 1 on error, 0 on success

### Verbose Mode
5. Verbose mode metadata display:
   - Use `rich.Panel` for metadata box
   - Show: latency_ms, tokens (prompt + completion), cost_usd, provider
   - Show smalltalk flag (True/False)
   - Style: dim gray

### Integration Tests
6. Integration tests in `tests/integration/test_cli_query_flow.py`:
   - **Test 1: Lore question end-to-end (smalltalk=false)**
     - Query: "Who is Roboute Guilliman?"
     - Assert: Response.answer contains relevant text
     - Assert: Response.personality_reply present
     - Assert: Response.sources not empty
     - Assert: Response.smalltalk == False
     - Assert: No error
     - Assert: Total latency <5 seconds
   - **Test 2: Smalltalk end-to-end (smalltalk=true)**
     - Query: "Hello there!"
     - Assert: Response.personality_reply present
     - Assert: Response.smalltalk == True
     - Assert: No error
   - **Test 3: Source URL validation**
     - Query: "Tell me about the Ultramarines."
     - Assert: All sources are valid wiki URLs
     - Assert: URLs contain "https://warhammer40k.fandom.com/wiki/"
     - Assert: No spaces in URLs (underscores only)
   - **Test 4: LLM API failure**
     - Mock OpenAI API to return 503 error
     - Assert: User sees friendly error message
     - Assert: Query logged with `error_occurred=True`
   - **Test 5: Hungarian and English language**
     - Query in Hungarian with --language hu
     - Query in English with --language en

### Benchmark Script
7. Performance benchmarking script in `scripts/benchmark_query.py`:
   - Run 10 test queries (mix of lore and smalltalk)
   - Record latency for each
   - Print p50, p95, p99 latencies
   - Print average cost per query
   - Print smalltalk vs lore ratio

### Documentation
8. Documentation updated:
   - `README.md` section: "Testing the RAG Query System"
   - Instructions: How to run CLI query command
   - Examples: 3-5 example queries (lore + smalltalk) with expected outputs

## Tasks / Subtasks

- [ ] Task 1: Create CLIAdapter class (AC: 1)
  - [ ] Create `src/adapters/` directory
  - [ ] Create `src/adapters/__init__.py`
  - [ ] Create `src/adapters/cli_adapter.py`
  - [ ] Set up typer CLI app
  - [ ] Add `if __name__ == "__main__"` entry point

- [ ] Task 2: Implement query command (AC: 2)
  - [ ] Define `query` command with typer
  - [ ] Add `question` argument (required)
  - [ ] Add `--language` option (choices: hu, en)
  - [ ] Add `--verbose` flag
  - [ ] Convert args to QueryRequest
  - [ ] Call QueryOrchestrator.process()

- [ ] Task 3: Implement rich output display (AC: 3)
  - [ ] Import rich console, Panel, Text
  - [ ] Format lore question output:
    - Answer text (default)
    - Personality reply (italic dim)
    - Sources header (bold blue)
    - Source URLs (clickable links)
  - [ ] Format smalltalk output:
    - Only personality_reply
  - [ ] Limit sources to 5

- [ ] Task 4: Implement error handling (AC: 4)
  - [ ] Catch exceptions from orchestrator
  - [ ] Print error message in red
  - [ ] Exit with code 1 on error
  - [ ] Exit with code 0 on success

- [ ] Task 5: Implement verbose mode (AC: 5)
  - [ ] Create metadata Panel
  - [ ] Show latency_ms, tokens, cost_usd, provider
  - [ ] Show smalltalk flag
  - [ ] Style as dim gray

- [ ] Task 6: Wire up dependencies
  - [ ] Initialize EmbeddingService
  - [ ] Initialize HybridRetrievalService
  - [ ] Initialize ContextExpander
  - [ ] Initialize MultiLLMRouter
  - [ ] Initialize ResponseFormatter
  - [ ] Initialize QueryLogger
  - [ ] Initialize QueryOrchestrator with all dependencies

- [ ] Task 7: Write integration tests (AC: 6)
  - [ ] Create `tests/integration/test_cli_query_flow.py`
  - [ ] Test lore question end-to-end
  - [ ] Test smalltalk end-to-end
  - [ ] Test source URL validation
  - [ ] Test LLM API failure handling
  - [ ] Test Hungarian language
  - [ ] Test English language
  - [ ] Mark tests with `@pytest.mark.integration`

- [ ] Task 8: Create benchmark script (AC: 7)
  - [ ] Create `scripts/benchmark_query.py`
  - [ ] Define 10 test queries (mix of lore/smalltalk)
  - [ ] Run queries and record latencies
  - [ ] Calculate p50, p95, p99 latencies
  - [ ] Calculate average cost
  - [ ] Calculate smalltalk vs lore ratio
  - [ ] Print results

- [ ] Task 9: Update documentation (AC: 8)
  - [ ] Add "Testing the RAG Query System" section to README.md
  - [ ] Document CLI command usage
  - [ ] Add 3-5 example queries with expected outputs

## Dev Notes

### Technical Background
- **CLI Testing:** Validates RAG pipeline before Discord complexity
- **Integration Tests:** Use real vector DB (Chroma test collection) and mocked LLM
- **Typer:** Provides excellent CLI UX with help text and type validation
- **Rich Library:** Makes output readable, professional, and engaging
- **Smalltalk Testing:** Validates natural conversation flow
- **Source URL Validation:** Ensures LLM generates proper wiki links

### CLIAdapter Example
```python
import asyncio
from typing import Optional

import typer
from rich.console import Console
from rich.panel import Panel
from rich.text import Text

from src.orchestration.query_orchestrator import QueryOrchestrator, QueryRequest

app = typer.Typer()
console = Console()

@app.command()
def query(
    question: str = typer.Argument(..., help="The question to ask"),
    language: str = typer.Option("hu", "--language", "-l", help="Response language (hu/en)"),
    verbose: bool = typer.Option(False, "--verbose", "-v", help="Show metadata"),
) -> None:
    """Query the WH40K lore bot."""
    asyncio.run(_query_async(question, language, verbose))

async def _query_async(question: str, language: str, verbose: bool) -> None:
    """Async query handler."""
    try:
        # Initialize orchestrator (with all dependencies)
        orchestrator = _create_orchestrator()

        # Create request
        request = QueryRequest(
            query_text=question,
            language_preference=language,
        )

        # Process query
        response = await orchestrator.process(request)

        # Handle error
        if response.error:
            console.print(f"[red]Error: {response.error}[/red]")
            raise typer.Exit(code=1)

        # Display response
        _display_response(response, language, verbose)

    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        raise typer.Exit(code=1)

def _display_response(response, language: str, verbose: bool) -> None:
    """Display formatted response."""
    if response.smalltalk:
        # Smalltalk: only personality reply
        console.print(Text(response.personality_reply, style="italic"))
    else:
        # Lore question: full response
        console.print(response.answer)
        console.print()
        console.print(Text(response.personality_reply, style="italic dim"))

        if response.sources:
            header = "ðŸ“š ForrÃ¡sok:" if language == "hu" else "ðŸ“š Sources:"
            console.print()
            console.print(Text(header, style="bold blue"))
            for url in response.sources[:5]:
                console.print(f"- {url}")

    if verbose:
        _display_metadata(response)

def _display_metadata(response) -> None:
    """Display metadata panel."""
    metadata = response.metadata
    content = f"""Latency: {metadata.get('latency_ms', 'N/A')}ms
Tokens: {metadata.get('tokens_prompt', 0)} + {metadata.get('tokens_completion', 0)}
Cost: ${metadata.get('cost_usd', 0):.6f}
Provider: {metadata.get('provider', 'N/A')}
Smalltalk: {response.smalltalk}"""

    panel = Panel(content, title="Metadata", style="dim")
    console.print()
    console.print(panel)

if __name__ == "__main__":
    app()
```

### Benchmark Script Example
```python
#!/usr/bin/env python
"""Benchmark script for query performance."""

import asyncio
import statistics
import time

from src.adapters.cli_adapter import _create_orchestrator
from src.orchestration.query_orchestrator import QueryRequest

TEST_QUERIES = [
    # Lore questions
    ("Who is Roboute Guilliman?", False),
    ("Tell me about the Ultramarines.", False),
    ("What is the Eye of Terror?", False),
    ("Who are the Primarchs?", False),
    ("What is the Horus Heresy?", False),
    ("Explain the Imperial Guard.", False),
    ("What are Space Marines?", False),
    # Smalltalk
    ("Hello!", True),
    ("Hi there", True),
    ("Hey", True),
]

async def run_benchmark():
    """Run benchmark queries."""
    orchestrator = _create_orchestrator()
    latencies = []
    costs = []
    smalltalk_count = 0
    lore_count = 0

    for query_text, expected_smalltalk in TEST_QUERIES:
        request = QueryRequest(query_text=query_text)

        start = time.perf_counter()
        response = await orchestrator.process(request)
        latency_ms = (time.perf_counter() - start) * 1000

        latencies.append(latency_ms)
        costs.append(response.metadata.get("cost_usd", 0))

        if response.smalltalk:
            smalltalk_count += 1
        else:
            lore_count += 1

        print(f"Query: {query_text[:30]}... | {latency_ms:.0f}ms | smalltalk={response.smalltalk}")

    # Calculate statistics
    latencies.sort()
    p50 = latencies[len(latencies) // 2]
    p95 = latencies[int(len(latencies) * 0.95)]
    p99 = latencies[int(len(latencies) * 0.99)]
    avg_cost = statistics.mean(costs)

    print("\n--- Results ---")
    print(f"p50 latency: {p50:.0f}ms")
    print(f"p95 latency: {p95:.0f}ms")
    print(f"p99 latency: {p99:.0f}ms")
    print(f"Average cost: ${avg_cost:.6f}")
    print(f"Smalltalk: {smalltalk_count}/{len(TEST_QUERIES)}")
    print(f"Lore: {lore_count}/{len(TEST_QUERIES)}")

if __name__ == "__main__":
    asyncio.run(run_benchmark())
```

### README Documentation Section
```markdown
## Testing the RAG Query System

### CLI Query Command

Test the RAG pipeline using the CLI adapter:

```bash
# Basic lore question (Hungarian)
poetry run python -m src.adapters.cli_adapter query "Ki az Roboute Guilliman?"

# English response
poetry run python -m src.adapters.cli_adapter query "Who is Roboute Guilliman?" --language en

# Verbose mode (show metadata)
poetry run python -m src.adapters.cli_adapter query "Tell me about the Ultramarines" --verbose
```

### Example Queries

**Lore Question:**
```bash
$ poetry run python -m src.adapters.cli_adapter query "Who are the Primarchs?"

The Primarchs are the twenty genetically-engineered sons of the Emperor of Mankind...

The Emperor protects.

ðŸ“š Sources:
- https://warhammer40k.fandom.com/wiki/Primarch
- https://warhammer40k.fandom.com/wiki/Emperor_of_Mankind
```

**Smalltalk:**
```bash
$ poetry run python -m src.adapters.cli_adapter query "Hello!"

Greetings, servant of the Imperium! How may I illuminate the darkness of your ignorance today?
```

### Performance Benchmark

Run the benchmark script to measure query performance:

```bash
poetry run python scripts/benchmark_query.py
```
```

### Relevant Source Tree
```
src/adapters/
â”œâ”€â”€ __init__.py                          # NEW: Module exports
â”œâ”€â”€ cli_adapter.py                       # NEW: CLI adapter
scripts/
â”œâ”€â”€ benchmark_query.py                   # NEW: Benchmark script
tests/integration/
â”œâ”€â”€ test_cli_query_flow.py               # NEW: Integration tests
README.md                                # UPDATE: Add CLI usage section
```

### Integration Test Strategy
- **Real Services:** Use real Chroma DB, real BM25 index
- **Mocked LLM:** Mock OpenAI/Anthropic to avoid API costs
- **Test Fixtures:** Pre-populated test collection with known articles
- **Markers:** Use `@pytest.mark.integration` for CI separation

### Exit Codes
| Code | Meaning |
|------|---------|
| 0 | Success |
| 1 | Error (validation, API, etc.) |

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-09 | 1.0 | Story created from Epic 2 | John (PM Agent) |

## Dev Agent Record

### Agent Model Used
_To be populated by dev agent_

### Debug Log References
_To be populated by dev agent_

### Pull Request
_To be populated by dev agent_

### Completion Notes List
_To be populated by dev agent_

### File List
_To be populated by dev agent_

## QA Results
_To be populated by QA agent_

---

**Estimated Effort:** 2.5 hours
